2 Executive/Summary/
The goal of this project was to develop adom   ain'specific'language (DSL) that could be 
used to build, train, and testar   tificial'neural'networks (ANNs). The DSL allows someone 
who is building an ANN to implement it in a language that focuses on the problem 
domain. This provides several benefits over using a generaDlpurpose language: 
¥ Ease of writing code 
¥ Speed of creating the solution 
¥ Clarity of the solution 
 
There are many existing libraries available to assist with building an ANN, but there 
currently did not exist a DSL. This project developed the DSL in such a way as to allow 
any library to be used by the DSL to implement the ANN (as long as the appropriate 
adapter for the library is supplied). The DSL was developed using the Scrum agile 
process. 
The DSL was evaluated using the DSL to solve realDworld problems with data from the 
UCI repository, by creating a new Tic Tac Toe playing agent in some existinso g ftware 
and by  informal  feedback  from  users  of  existing  ANN  libraries .The  result  of  the 
evaluation showed that the DSL was a practical and useful tool capable of real world use. 
The evaluation also showed where the next stage of development should focus .
 
 
The key achievements of the project are: 
¥ The development and release of the DSL for ANNs, called Neural (see sections 7 
& 8) 
¥ The creation of a design pattern for providing a singlein   terface (an application 
programming interface or API) over multiple libraries, each with different APIs 
(see section 9.5) 
¥ Establishing an open source project forNe   ural, that will exist beyond the lifetime 
of this product (see section10.   2) 
 
 
 
 
2 3 Contents/
1  Acknowledgements/...................................................................................................../1 
2  Executive/Summary/....................................................................................................../2 
3  Contents/....................................................................................................................../3 
4  Introduction/................................................................................................................./5 
4.1  Document Structure  .............................................................................................................................. 5 
4.2  Conventions & Terminology .............................................................................................................. 5 
5  Goals/.........................................................................................................................../6 
5.1  Aims ............................................................................................................................................................. 6 
5.2  Objectives .................................................................................................................................................. 6 
5.3  Deliverables  .............................................................................................................................................. 6 
5.3.1  The'Domain'Specific'Language'.....................................................................................................'6 
5.3.2  An'Available'Resource'.......................................................................................................................'6 
5.4  Added Value .............................................................................................................................................. 6 
6  Background/................................................................................................................../7 
6.1  Artificial Neural Networks ................................................................................................................. 7 
6.1.1  Fundamentals'.......................................................................................................................................'7 
6.1.2  Learning'in'Artificial'Neural'Networks'..................................................................................'11 
6.1.3  Developing'an'Artificial'Neural'Network'..............................................................................'14 
6.1.4  Other'Types'of'Artificial'Neural'Networks'............................................................................'15 
6.1.5  Summary'..............................................................................................................................................'16 
6.2  A Survey Of Current Tools ................................................................................................................ 16 
6.2.1  Introduction'.......................................................................................................................................'16 
6.2.2  Why'Use'Existing'Libraries'&'Tools?'.......................................................................................'16 
6.2.3  Libraries'for'Artificial'Neural'Networks'................................................................................'16 
6.2.4  Tools'for'Implementing'the'Domain'Specific'Language'.................................................'19 
6.2.5  Summary'..............................................................................................................................................'19 
6.3  Domain Specific Languages ............................................................................................................. 19 
6.3.1  Introduction'.......................................................................................................................................'19 
6.3.2  What'Is'A'Domain'Specific'Language?'...................................................................................'19 
6.3.3  Why'Use'a'Domain'Specific'Language?'..................................................................................'20 
6.3.4  Building'a'DSL'...................................................................................................................................'21 
6.3.5  What'Do'We'Want'To'Build'With'The'DSL?'........................................................................'22 
6.3.6  Summary'..............................................................................................................................................'23 
6.4  Related Work ......................................................................................................................................... 24 
7  Designing/and/Implementing/the/DS/................................ L ........................................./25 
7.1  The Approach ......................................................................................................................................... 25 
7.1.1  Test'Driven'Development'.............................................................................................................'26 
7.1.2  Tools'.......................................................................................................................................................'27 
7.2  The Development Story ..................................................................................................................... 27 
7.3  The Neural Language .......................................................................................................................... 30 
7.3.1  Language'Definition'.......................................................................................................................'31 
7.3.2  Network'Definition'..........................................................................................................................'31 
7.3.3  Activation'Definition'.......................................................................................................................'32 
7.3.4  Training'................................................................................................................................................'32 
7.3.5  Testing'..................................................................................................................................................'33 
7.3.6  A'Full'Example'...................................................................................................................................'33 
7.4  The Architecture of Neural .............................................................................................................. 34 
 
3 7.4.1  The'Neural'Platform'.......................................................................................................................'34 
7.4.2  The'Neural'API'..................................................................................................................................'35 
7.4.3  Integrating'Underlying'Libraries'..............................................................................................'36 
7.5  Summary .................................................................................................................................................. 36 
8  Results/......................................................................................................................./37 
8.1  Using Neural ........................................................................................................................................... 37 
8.1.1  Create'a'network'to'use'programmatically'.........................................................................'37 
8.1.2  Create'and'train'a'network'to'use'programmatically'.....................................................'37 
8.1.3  Create,'train,'and'test'to'use'programmatically'................................................................'38 
8.1.4  Command'Line'use'...........................................................................................................................'38 
8.1.5  Errors'....................................................................................................................................................'39 
8.1.6  Changing'the'ANN'library'............................................................................................................'40 
8.2  The Scope of Neural ............................................................................................................................ 40 
8.3  Extending Neural .................................................................................................................................. 41 
8.3.1  Extending'The'ANN'Library'Coverage'....................................................................................'41 
8.3.2  Implementing'New'Underlying'Libraries'..............................................................................'43 
8.4  Summary .................................................................................................................................................. 43 
9  Analysis/&/Evaluation/................................................................................................./44 
9.1  Examples From Neuroph/UCI ........................................................................................................ 44 
9.2  Building An Agent for Tic Tac Toe ................................................................................................ 46 
9.3  Feedback from users ........................................................................................................................... 47 
9.4  What Should Be Changed or Added ............................................................................................. 47 
9.5  Neural As A Design Pattern .............................................................................................................. 48 
9.6  Lessons Learned ................................................................................................................................... 49 
9.6.1  Do'not'underestimate'working'with'libraries'.....................................................................'49 
29
9.6.2  No'battle'plan'survives'contact'with'the'enemy '.............................................................'50 
9.6.3  The'Curse'Of'Generality'.................................................................................................................'50 
9.6.4  Benefits'of'TDD'..................................................................................................................................'51 
9.7  Summary .................................................................................................................................................. 51 
10  Summary/................................................................................................................./52 
10.1  Achieving The Goals ......................................................................................................................... 52 
10.2  Future Plans ......................................................................................................................................... 53 
11  References/.............................................................................................................../54 
12  Web/Resources/......................................................................................................../56 
12.1  ANN Libraries ...................................................................................................................................... 56 
12.2  Language Resources ......................................................................................................................... 56 
12.3  Other ....................................................................................................................................................... 56 
13  Abbreviations/........................................................................................................../57 
14  Appendix/................................................................................................................./58 
14.1  Neural API ............................................................................................................................................. 58 
14.2  Tic Tac Toe Agent .............................................................................................................................. 58 
14.3  A Very Short Summary of Scrum ................................................................................................ 61 
14.3.1  Basic'Concept'..................................................................................................................................'61 
14.3.2  User'Stories'......................................................................................................................................'61 
14.3.3  Backlogs'............................................................................................................................................'61 
14.3.4  Estimation'&'Velocity'..................................................................................................................'61 
14.3.5  Burndown'Charts'..........................................................................................................................'61 
14.3.6  The'Process'.......................................................................................................................................'62 
14.3.7  Summary'...........................................................................................................................................'62 
 
 
4 4 Introduction/
The aim of this project is to design and implement a domain specific languag e(DSL)  for 
researching, creating, and using artificial neural networks (ANNs). This language will be 
used for quick and easy implementation of ANNs by the domain experts (for example, 
scientists). It will also allow the design and implementation of new artificial neural network 
algorithms to be described concisely and precisely. 
4.1 Document/Structure/
This document is laid out in the followingke   y parts: 
¥ Goals: Defining the aims and objectives for the project 
¥ Background: Giving the reader some background on ANNs & DSLs 
¥ Designing1and1Implementing1The1DSL: Describing the development of the 
DSL, and the code created1
¥ Results: Showing the scope of what was created, together with a description of 
DSL, and how the DSL should be used 
¥ Analysis1&1Evaluatio: nEvaluation of the DSL with real world examples, and 
actual users, together with an analysis of the project .
4.2 Conventions/&/Terminology/
Within this document, code fragments are presented in the following form :
 
public'void'helloWorld();'
 
These are left inline to preserve the narrative .
Where full examples, or larger fragments are needed a figur eis used, such as: 
 
public'class'HelloWorld'{'
' public'void'hello()'{'
' ' System.out.println(ÒHello'world Ó);'
' }'
}'
Figure111Example1
 
5 5 Goals/
5.1 Aims/
The  aim  of  this  project  is  to  design  and  implement  a  domain  specific  language  for 
researching, creating, and using artificial neural networks. This language will allow noDn
software engineers to quickly and easily implement artificial neural networks. It will also 
allow the design and implementation of new artificial neural network algorithms to be 
described concisely and precisely. 
5.2 Objectives/
The following objectives will be met during the course of the project :
¥ Review the current state of software and tools for neural networks specifically 
and machine learning in general 
¥ Evaluate the needs for a domain specific language 
¥ Design and implement the domain specific language 
¥ Test the language against real world problems by implementing artificial neural 
networks 
¥ Evaluation of the language by itÕs potential user s
¥ Make the language available for use by other s
5.3 Deliverables/
5.3.1 The/Domain/Specific/Language /
The exact scope of the language is to be decided during the project, dependent on the 
outcome of the evaluation phase. However, at minimum, it will be in the form of a 
declarative language capable of expressing the design of an artificial neural network in a 
concise  and  precise  manner.  Optionally  it  will  also  provide  the  ability  to  define 
experiments to both train and use the artificial neural network. A further optional 
deliverable is to generalise the language such that it can be extended to other areas of 
machine learning, such as evolutionary algorithms or reinforcement learning .
5.3.2 An/Available/Resource/
Just creating the language is not enough, it needs to be available for others to use. The 
implementation of the language will be open sourced and hosted online using a service 
such as SourceForge, or GitHub. Furthermore a plan will be put in place for maintenance 
and further development of the language .
It is also the intention to have a paper describing the language published so that it can 
be found, used and referred to in future researh. c  
5.4 Added/Value/
This project aims to fill two needs.  Firstly to provide a platform for implementing and 
using  artificial  neural  networks,  without  focussing  on  the  software  engineering. 
Secondly to provide researchers with a tool that allows them to descrie  bnetworks and 
algorithms  in  a  concise  and  precise  manner,  something  that  is  unavailable  at  this 
moment.  
This  project  provides  the  base  for  developing  a  standard  base  of  domain  specific 
languages that fill similar needs across a range of computer sciencedi sciplines. 
The value of this project will be shown by the interest and uptake for others within the 
scientific community. 
 
 
6 6 Background/
6.1 Artificial/Neural/Networks/
This section gives an overview of artificial neural networks from an end userÕs point of 
view. It aims for a broad coverage, rather than narrowing down into any one area . 
The key goal is to be able to understand artificial neural networks enough so that their 
use  and  implementation  can  be  considered  and  a  domain  specific  language  can  be 
designed. 
Initially we focus on the fundamental concepts of artificial neural networks, and proceed 
to  look  at  some  more  complex  networks.  Finally  we  will  consider  the  process  of 
implementing an artificial neural network .
1
This section is mostly sourced from Haykin, and it should be assumed that the material 
originates from this source, unless otherwise referenced. It is also assumed that the 
reader has some familiarity with artificial neural networks already .
6.1.1 Fundamentals/
Artificial'neural networks (or ANNs) are computer programs inspired by biological 
nervous  systems,  or  specifically,  brains.  A  brain  works  completely  differently  to  a 
1
computer processor, and to quote Haykins, brains are a Òhighly'complex,'nonlinear,'and'
parallel'computerÓ. An ANN is effectively a software simulation of biological hardware. 
Mostly we are not too interested in retaining a true simulation of the biology and are 
only interested in using the concepts for computational purposes. Section 0 discusses 
the relationship between ANNs and Neuroscience .
While a computer works by executing a set of instructions for loading, manipulating and 
storing numbers in a memory, an ANN works as a network of nodes with each taking 
input and under various conditions, producing output (for example if the input reaches a 
certain threshold). ANNs are inherently learning machines, that is, they improve their 
performance in a certain task over time, with experience. They achieve this by modifying 
the strengths of the connections between the nodes .
2
An ANN can be proved to be Turing complete, that is, it is capable of performing any 
computable algorithm. This means it can perform any task a normal computer processor 
can, however the tasks for which it is suited may be different, and some of these are 
examined further in section 0. 
A"Biological"Neuron"
A neuron is a single cell found in the nervous systems and is the fundamental unit from 
which brains are built. It is also the fundamental unit of computation in the brain. 
Although there are many types of neurons, a typical neuron can be seen iFi n gure 2 and 
its behaviour described as following: 
1. The neuron receives an electrical current through the dendrites 
2. When the electrical potential in the cell passes a particular threshold the neuron 
fires 
3. The neuron transmits an electrical current through itÕs axons  
4. The axons of one neuron are connected to other neuronÕs dendrites throug h
synapses, which form a chemical bridge between the two. The synaptic bridge 
can be regulated by chemicals in the brain know as neurotransmitter s.
5. By analogy to software programs, the dendrites can be thought of as input, the 
axons as output, and the neurotransmitters as functioning as some sort of global 
control over behaviours. 
 
 
7  
Figure121A1typical1neuron1found1in1a1brain1(image1from1WikiMedia1Creative1Com 1 mons)
 
  
 
Figure131A1basic1model1of1an1artificial1nneu 1ro
 
   
 
8  
A"Basic"Model"of"an"Artificial"Neuron"And"An"Artificial"Neural"Network"
 
The model shown in Figure 3 is used as a basis for most neurons in ANNs. Each input 
signal x is modulated by a synaptic weightw   , and then the inputs summed so that the 
input to a neuron is: 
 
 
 =        
   
   
 
The activation function is then applied to generate the output :
 
 =  ?( ) 
 
A simple example of an activation function is a threshold function :
 
1  "  >  
?   =   
0  "  ²  
 
where a neuron would provide an output signal of 1 if the weighted sum of the inputs 
exceeds some threshold t. 
With this basic model, each neuron can be fully described by the following parameter s:
¥ The weighting of each synapse (w) 
¥ The activation function (?) 
'
An artificial neural network is then created from a set of neurons with connections 
between  them.  Connections  can  be  from  input  to  the  network,  to  output  from  the 
network,  or  between  the  inputs  and  outputs  of  neurons  within  the  network.  The 
network can then be fully described by the following  parameters: 
¥ Input connections  
¥ Connection between neurons 
¥ Output connections 
Activation"Functions"
1
The activation function defines the output of a neuron given a particular input. Haykins 
describes three basic types: 
¥ Threshold: where the output is zero until an input threshold is reache d
¥ Piecewise1 linear:  where  output  is  linearly  dependent  on  the  input  until  a 
maximum is reached 
¥ Sigmoidal: where there is a nonDlinear relationship, an example of which might 
be the tanh(u) function'
However from a software engineerÕs point of view, any function could be used that 
produces output given an input (obviously, with varying degrees of usefulness). An 
example  of  a  more  complex  function  might  be  a  stochastic  (probabliistic)  function 
designed to simulate the noisiness of a real neural network.  
   
 
9  
Types"of"Networks"
There are three basic types of ANN architecture :
SingleVlayer/Feed/Forward/
In a layered architecture, the neurons form layers whose output maps to the inpuof t  the 
next layer, and never vice versa (hencefe   ed'forward). In the singleDlayer feed forward 
network there is only one layer of neurons providing output .
Figure141A1singleOlayer1feed1forward1netwo1 rk
 
MultiVLayered/Feed/Forward/
A multiDlayered architecture consists of one or more hidden layers. Te  h hidden layers 
allow  extra  dimensions  of  neuron  interactions,  and  increase  the  capabilities  of  the 
network. 
Figure151A1multO ilayer1feed1forward1netwo1 rk
 
 
10 Recurrent/Network/
A recurrent network is one is which has one or more feedback loops. This allows the 
network have a dynamic state. Typically these networks involve the use of unit delay 
elements, which can result in nonlinear behaviour .
 
The"Relationship"With"Neuroscience"
Since an artificial neural network is an abstract model of a biologic network, it also 
provides  benefits  to  neuroscience.  Neuroscience  also  uses  artificial  neural  network 
models but in a different way .
I have classified the two differently based on their goals :
1. Neuroscience Model: where the goal is to simulate some aspect of biology in 
order to understand it 
2. Artificial Neural Network: where the goal is to perform some task (with the ANN 
as just a means to an end )
 
The first focuses on representing some biological truth, while the second is freer to 
abstract, or even to have abilities/parameters that are not present in biology.  
Given  the  similarities  between  both,  it  is  worth  keeping  both  options  open  when 
considering software for artificial neural networks .
 
6.1.2 Learning/in/Artificial/Neural/Network/s
We have talked about how an ANN operates, based on the weights of the synapses. But 
how do we find those weights? The network must start with some default (perhaps 
random) set of weights and changes are made through training to successively improve 
the weights. We call this learning .
The following sections give an overview of the learning agor l ithms that may be used for 
an ANN. 
Hebbian""
The  Hebbian  learning  rule  is  one  of  the  oldest  learning  rules  for  ANNs.  It  can  be 
Figure161A1recurrent1networ1 k
 
11 characterised in the following two stages applied to each input into the neuron  :
1. If both input and output to the neuron are aciv te then increase the weighting 
2. If just one of the input or output is active then decrease the weigh t
 
In itÕs simplest, this can be visualised mathematically with the following :
Æ  =          
     
where Æ   is the change to the weight,    and    the output and input to a neuron, and   
     
a constant representing the learning rate .
Back"Propagation"
Back propagation is a common method of learning. Given a supevi rsed training set, the 
network input is calculated and then values fed forward through the network to provide 
the  output  values.  The  deltas  between  the  actual  and  expected  output  arebac   k'
propagated through the whole network. Each weight can then be updaed t  according to 
the learning rate. 
Boltzmann"
The  Boltzmann  rule  is  a  stochastic  learning  rule  based  on  ideas  from  statistical 
mechanics. A Boltzmann machine is an ANN that uses the Boltzmann learning rul e.
 
1
 =  ?       '
 "    
2
   
'
Where    is the state of neuronj  and    is the synaptic weight betweeni  and j. 
   "
The  learning  rule  operates  by  picking  a  neuron  at  random  and  flipping  it  with 
probability: 
 
1
    ??  =   '
   
 Æ 
 
 
1+ 
'
where Æ   is the energy change that would result from the flip, and T represent the 
 
temperature, in this this case being a tuneable parameter of the system  .
A Boltzmann machine is a specific instance of asto   chastic'machine. 
 
Supervised"versus"Unsupervised"Training"
There are two forms of training that can occur with artificial neural networks, or indeed 
with machine learning in general. These are Ôsupervised trainingÕ and Ôunsupervised 
trainingÕ. 
Supervised training consists of using training data that corresponds to a labelled output 
set.  An  example  of  this  might  be  classification  of  pictures  into  animals,  where  the 
training  set  would  consist  of  pictures  of  animals  together  with  their  correct 
classification.  
In unsupervised training there is no knowledge of correct outputs. The network must 
derive itÕs own analysis of the input data. An example of this might be a network that 
makes time series predictions on an input signal . 
 
Cross"Validation"
Ideally training should happen with a large training data set, and then the resultant 
trained network can operate on a statistically similar test data set, before being used 
Òfor realÓ. The purpose of the test data set is to measure the accuracy of the trained 
 
12 network. Since we want to train on as much data as possible, using a separate test set 
may be undesirable. 
Cross validation is a technique that maximises our training data while providing a useful 
test data set. It works in the following way :
1. The data is partitioned into random subsest 
2. For each subset X the ANN is trained on every subset apart from X and then 
tested using X 
 
Tasks"for"Artificial"Neural"Networks"
Artificial neural network are capable of many things (indeed, since they are Turing 
complete they can perform any computable operation). It is possible to reduce the 
problems for which they are efficient down to a few broad categories. Haykins reduces it 
down to 5 which are described below plusBe   amforming which I consider part of Data'
Processing. There are, of course, other appic l ations across a broad range of problem 
domains. 
Pattern"Association"
Pattern association is the task where after being supplied with one form of input, the 
network will provide an associated output. In autoZassociation the output pattern is the 
same as the input pattern, and the input pattern may be a noisy or partial pattern. In 
heteroZassociation the input and output patterns are different, and this could be thought 
of as a keyDvalue pair. 
Pattern  association  is  typically  unsupervised  learning  with  the ra t ining  being  the 
storage of patterns, and the testing being the recal l.
Pattern"Recognition"
Pattern recognition is the task where after being supplied with some input, the network 
will classify it into one or more types. This is typically a supervised trinaing where the 
network  is  initially  trained  with  examples  of  each  class,  and  then  during  testing 
presented with a new pattern it must classify correctly .
Although Haykins called this Pattern'Recognition,'I prefer the term Classification which 
seems more prevalent through the machine learning literature .
'
Function"Approximation"
The output of an artificial neural network can be considered a nonlinear function of the 
input: 
 =  ( ) 
where d1is the output, given inputx . 
For a function approximation task, the ideal functionF  must be approximated, but is not 
known. Instead training data supplying examples ofd1   and x is used to allow the network 
to learn an approximation toF , such that: 
 
    ? ( ) <    
 
where   is a small positive number .
Control"Systems"
Controlling a physical system (or it may be an external system of any kind) is another 
task type for artificial neural networks. In this task a system would take contr osilgnals 
and provide some form of feedback in return. The ANNco   mpares the feedback signal 
with the reference signal to control the system in order to keep it in line with the 
required behaviour (as described by the reference signal). 
 
13 Figure171Typical1architecture1fr1 o an1artificial1neural1network1controlling1an1external1s ystem
'
 
Data"Processing"
Haykins classes this as Filtering, however I prefer to think about filtering as a subset of 
Data'Processing. The difference is just the terminology, but I think the original use of the 
term  ÔfilteringÕ  was  misleading.  Data  Processing  is  any  task  in  which  the network 
receives some input data and performs some kind of analysis or manipulation of it. This 
is a wideDranging class of task, and I think Haykins is using it as a bit of a catch Dall for 
everything left at the end .
Subcategories of this task include: 
¥ Filtering:  extracting  signals  from  within  noisy  data  or  from  other  masking 
signals 
¥ Compression: reducing the size of a data set while maintaining the amount of 
information 
¥ Prediction: receiving a sequence on input data and predicting future data based 
on this 
 
 
6.1.3 Developing/an/Artificial/Neural/Networ/k
Design"
3
According to Senyard two key design decisions typically influence the development of 
an ANN. These decisions are not software design d ecisions, instead they are problem 
domain decisions driven purely by the particular problem to be solved:  
¥ Which network architecture to use (e.g. multDilayer feed forward) 
¥ Which learning algorithm to use 
3,4
Following the key design decisions the next set of ec d isions are typically : 
¥ Parameters controlling the structure of the network (e.g. number of layers )
¥ Learning algorithm parameters 
¥ Training and testing strategy 
¥ Error levels 
The  results  of  the  decisions  above  cover  most  of  the  needed  parameters  to  fully 
implement an ANN. 
Software"Development"of"an"Artificial"Neural"Network"
4
Rodvolds  suggests an ANN development process that has the following steps:  
1. Network requirements, goals, & constraints 
2. Data gathering and preDprocessing 
3. Training and testing loops 
4. Network Deployment  
5. Independent testing & verification 
 
14 where stage 3, training and testing loops, is able to try the following variation s:
¥ Variation of ANN topologies (i.e. architecture) 
¥ Variation of ANN paradigms (i.e. learning algorithms, connection etc. )
¥ Selection and combination of ANN input neurons 
This process shows that the focus here is on the ANN and the data (i.e. the problem 
domain) and not on software engineering. 
6.1.4 Other/Types/of/Artificial/Neural/Networ/ks
Section 6.1.1 describes several fundamental types of artificial neural networks, however 
other types of networks exist. This section describes a set of more complex networks 
and some of their properties that need to be considered when designing an ANN of that 
type. There are obviously more than these, but these have been selected to give a flavour 
of what might be encountered. 
Radial"Basis"Function"Network"
A Radial Basis Function, or RBF, network is an artificial neural network that uses radial 
basis functions as the activation function for the neuron. The network takes the form of 
a multiDlayered feed forward network, with the hidden layer containing linear radial 
basis functions. It is typically used for :
5
¥ Time series prediction 
¥ Function approximation 
6
¥ Control Systems  
When designing and implementing these networks, additional consideration needs to be 
given to extra parameters used by the radial basis functions (the centre vectors, and RB F
width). 
Dynamic"Recurrent"Networks"
Recurrent networks can have feedback that is either local or global. Dynamic recurrent 
networks are recurrent networks with global feedback. Like other recurrent networks 
they use unit delays, but will involve feedback nt i o the input. 
Although  a  discussion  of  their  properties  is  beyond  the  scope  of  this  paper,  the 
interesting properties from a software point of viewar   e: 
¥ Unit delays 
¥ Feedback into the input layer 
Although Haykins positions this as a separate topic from th ebasic recurrent network 
(and in terms of mathematical analysis, it is), for our needs it has similar requirements 
to the basic recurrent network described in section6.  1.1 
Committee"Machines"
A committee machine consists of more than one artificial neural network. Each of these 
processes the same test data and the results are combined to produce a single output   .
Examples of committee machines are: 
¥ Ensemble averaging Ð otherwise identical networks are initialised differently, 
trained on the same data set and the output averaged. The result is slightly 
7
better than the average of the individual networks 
¥ Boosting Ð identical networks are trained on different data sets. The results of 
the  test  data  are  combined.  The  overall  result  is  that  the  output  from  the 
committee  machine  performs  as  a  much  better  algorithm  than  any  of  the 
individual machines on their own  
 
Evolutionary"Artificial"Neural"Networks"
Evolution  is  another  method  of  learning.  It  can  be  combined  wit hartificial  neural 
8
networks in three ways : 
 
15 ¥ Weights 
¥ Connections & architecture 
¥ Learning rules 
In  each  of  these  cases  the  information  is  encoded  into  a  genotype  and  normal 
evolutionary algorithms are used. When calculating the fitness of the neural network, it 
is trained on a training set, then tested as normal, with the difference between the tu acal 
and expected outputs forming the fitness .
9
There are two problems to overcome when implementing an evolutionary ANN:  
¥ Noisiness due to random initial weights can mean the fitness measure is not 
accurate. This can be overcome by training many networks with different initial 
weights, but this is computationally expensive .
¥ Encoding of parameters into a suitable genome (although I on c sider this to be a 
problem for any evolutionary algorithm) 
 
6.1.5 Summary/
In this section of the paper, we have had a flying overview of artificial neural networks. 
It  has  not  been  necessary  to  understand  all  of  artificial  neural  networks  at  an 
implementation level, as the intention for this project is to reuse existing ANN libraries. 
As such developing a domain specific language relies more on the functionality of the 
ANN  libraries.  The  intention  has  been  to  show  the  range  o fnetwork  architectures 
available  and  some  of  the  development  decisions  that  are  made  during  their 
implementation.  
 
 
 
6.2 A/Survey/Of/Current/Too/ls
6.2.1 Introduction/
This  section  looks  at  existing  libraries  and  tools  available  or  both  artificial  neural 
networks  and  for  implementing  domain  specific an l guages.  Initially  we  discuss  the 
reasons for reusing such libraries & tools, and thn e goes on to discuss and compare the 
libraries and tools available for both ANNs and DSLs. Finally there will be a short 
discussion of PMML, a markDup language used to specify ANNs, as well as other machine 
learning algorithms & models. 
6.2.2 Why/Use/Existing/Libraries/&/Tool/s?
The focus of this project is about building a domain specific language for artificial neural 
networks, and not about building a reusable library to implme e nt ANNs. As such it is 
better to reuse libraries if they are available . 
The reuse of libraries is good because :
¥ Reuse of existing code: reducing the time taken to implement a system. This 
allows time to be spent on the real problems instead of implemenintg things that 
have been done before. 
¥ Greater stability and reduced risk: all the code in the library has already been 
through entire development cycles, and should be stable & tested .
¥ Choice of implementation: if designed properly, the library can be swppe a d for 
another, providing a choice of implementation. Different implementations with 
different attributes can be used as required. 
6.2.3 Libraries/for/Artificial/Neural/Network/s
This section discusses five existing libraries that can be used for implementing rtaificial 
neural networks. It then compares the features of each.  
 
16 Listing"of"Artificial"Neural"Network"Libraries"
Links to the websites of all these libraries can be found in Section 12.   1. 
PyBrain/
PyBrain is an open source machineDlearning library written in the Python language. It 
focuses on ANNs, reinforcement learning, and optimisation. It aims to be an easDytoDuse 
and modular framework, suitable for both students and researchers .
PyNN/
PyNN is a library used to create neuronal network models (biological) written in Python. 
It provides the ability to run the models on various backend libraries, all of which are 
focused on simulating biologic neural networks. PyNN doesnÕt provide the ease of use 
Feature1 PyBrain1 PyNN1 Encog11 Neuroph1 dANN1
ANN1Basics1 Full  Full  Full  Full  Full 
Activation1Function1Librar1y Good  Some  Full  Good  Good 
Feed1Forward1Network1s Full  Some  Full  Good  Good 
Recurrent1Networks1 Full  Some  Full  Some  D 
Boltzmann1Machines1 Good  D  Good  Good  D 
Self1Organising1Map1s Good  D  Good  Good  D 
Neuroscience1Models1 D  Good  D  D  Some 
Other1Network1Model1s Good  D  Good  Some  Some 
Other1Machine1Learning11 Good  D  Some  D  Some 
Supervised1Training1 Good  D  Good  Some  Some 
Unsupervised1Training1 Good  D  Good  Some  Some 
Reinforcement1Learning1 Good  D  Good  Some  Some 
Other1Learning1Method1s Good  Some  Good  D  D 
Data1Models1For1Input/Outp1ut D  D  Good  D  D 
Optimisation1 Some  D  Some  D  Some 
Genetic1Algorithms1 Some  D  Good  D  Good 
Testing/Training1 Good  D  D  D  D 
Environments1
GPU1Execution1 D  D  Some  D  D 
Visualisation1 Some  D  D  D  D 
 
Table111A1comparison1of1the1main1features1of1several1ANN1libr 1 aries
 
 
17 that PyBrain does, but it does provide a gateway into the biological simulation side of 
neural networks. 
Encog/
Encog  is  a  library  for  artificial  neural  networks  and  machine  learning.  It  has  two 
versions, one in Java and the other in C#. Encog has a wide variety of network models, 
activation functions and learning techniques built into it. Encog also provides the ability 
10
to run on GPUs (Graphical Processing Units) through the use of OpenCL. 
Neuroph/
Neuroph is a lightweight framework for building artificial neural networks, written in 
Java. It provides a small number of powerful base classes for implementing ANN s.
dANN/
dANN is a library for artificial intelligence and genetic algorithms, as well as machine 
learning in general. It is written in Java and boasts the widest range of buDilin t  abilities, 
however within ANNs itÕs range may be limited. 
 
Table 1 shows a comparison of the features of interest to this project for each library . 
Using"Multiple"Libraries"
Different ANN libraries offer different features. Even libraries that offer less features 
than other libraries, may outperform them in the tasks that they do. So choosing a 
library on which to base a solution is difficult, without no k wing how that solution will be 
used. 
11
Fortunately, judicious use of design patterns  can ensure that the decision of which 
library to use is left until run time.Fi   gure 8 shows how the Factory and Adapter design 
patterns  can  be  used  to  break  the  coupling  between  the  DSL  and  the  ANN 
implementation library. The DSL uses a LibraryAdapterFactory to load an object of type 
LibraryAdapter. It doesnÕt know what the concrete type is and doesnÕt care, te  hactual 
type would probably be controlled through a configuration parameter. Each ANN library 
then has itÕs own LibraryAdapter to adapt from the interface required by the DSL to how 
the library is implemented. To add another library to this pattern wouldon   ly take the 
implementation of a new LibraryAdapter .
 
 
 
Figure181A1design1for1the1capability1of1using1multiple1ANN1lib1raries
 
18 6.2.4 Tools/for/Implementing/the/Domain/Specific/Langua/ge
For implementing the DSL there are two main decisions to make :
¥ What programming language to develop the DSL with 
¥ What support tools to use 
Each of the decisions can affect the other.  
Language""
For the implementation language, the libraries for ANNs constrained the choice to just 
Java & Python and both of these languages can integrate with libraries written in the 
other language. Given that I have 13 years experience as a Java software engineer, 
existing skills weigh heavily in favour of Java .
Equally, looking at the tools available in each language, there is a wider range available 
in Java than in Python .
Parser"
Web URLs for these tools can be foundin    Section 12.2. 
12
JavaCC (Java Compiler Compiler) is a compiler compiler  that generates Java code from 
13
a  Backus  Naur  Form  (BNF)  grammar  specification.  The  code  generated  forms  an 
interpreter that can read scripts in the target language and execute desired Java code. In 
the initial generated form there are only stubs for later expansion. 
14
JParsec is a recursive descent parser . It does not use a grammar specification to define 
the syntax, instead relying on Java objects that are created and combined together . 
The end result of using both tools is the same: a Java library that will parse and execute 
the DSL scripts, so the decision of which to use is a matter of taste.  
6.2.5 Summary/
We have seen a range of libraries that could be used for building ANNs. They range from 
simple to extensive, and have various strengths and weaknesses. Since we do not want 
to commit to a single library, we want to allow th e DSL to use a library that can be 
selected at runtime, and we have seen the design patterns that will allow thi s.
The implementation language of choice is Java, and the particular tool to be used to 
implement the DSL has not yet been chosen but the decison i  will not affect the project 
significantly. 
 
6.3 Domain/Specific/Languages/
6.3.1 Introduction/
This section will describe what a domain specific language (DSL) is, why we should use 
one, and how it should be implemented. It will go on to discuss the key features  of the 
DSL that will be developed. 
6.3.2 What/Is/A/Domain/Specific/Languag/e?
A domain specific language is a programming language that is designed to be used for 
programming  in  a  particular  problem  domain.  Normal  languages  such  as 
Java/C/FORTRAN  can  be  thought  of as general'purpose'languages (GPLs),  and  they 
provide the abilities to easily work through most computable problems. A DSL instead 
focuses on making it as easy as possible to solve problems in the specific problem 
domain for which they are designed. Some examples of DSLs can be seen in Table 2. 
 
19 Domain1Specific1Languag1e Purpose1
HTML1 Description of the presentation of web pages 
SQL1 Querying and controlling databases 
COBOL1 Programming within the business domain 
ESDL1 Describing evolutionary algorithms 
1
Table121Some1examples1of1domain1specific1langua1ges
 
DSLs may range from languages such as HTML, which are highly specialised towards 
their problem domain, and not capable of general computing, to languages such as 
COBOL, which are arguably generalDpurpose languages. 
15
Programming languages could be visualised on a horizontal scale, with DSLs on the far 
left and GPL such as C on the right. Although the real properties of languages encompass 
many more dimensions than this, the analogy is apt. There is no definite border between 
a GPL and a DSL, even though the extremes are clear.  
6.3.3 Why/Use/a/Domain/Specific/Languag/e?
15
There are several reasons for using a DSL over writing the code in a GPL.  
The first is that a DSL provides a higher level of abstraction over the problem that is 
being  solved.  This  means  the  code  is  simpler,  and  simpler  code  means  less  code, 
something all software engineers strive for. 
The second reason is that a DSL provides a large amount of reuse. This comes from the 
high level of abstraction, which results in less low level code being (or needed to be) 
written. 
Thirdly, because of the high level of abstracio t n, the DSL is much easier to use for a 
problem domain practitioner. The practitioner may not even be a software engineer 
(and this is one reason to develop a DSL), so making it easy for someone to write code is 
very important. 
Finally, the resultant code solution is highly communicable. Because of the high level of 
abstraction the code is aligned with the problem domain, not general computing, so it is 
easy for practitioners to communicate about the problem domain using the DSL .
Figure 9 shows two pieces of code, the first in Java, and the second in a hypothetical DSL. 
Note the difference in readability and clarity of the problem in the second piece of code 
compared to the GPL version .
'
'
//In'Java'
Transaction'transaction'='transactionManager.begin();'
withdraw(transaction,'ÒUSDÓ,'95552000,'1000,'true);'
deposit(transaction,'ÒUSDÓ,'97893451,'1000,'true);'
transaction.complete();'
'
'
//In'BankingDSL'
transfer'1000'in'USD'from'95552000'to'97893451'
Figure191Comparison1of1well1factoredav1aJ1with1hypothetical1banking1D SL
'
'
Human"&"Computer"Roles"Ð"A"Justification"For"DSLs"
What roles do a human and a computer play during the development of a piece of 
software? At a coarse granular level, a human writes code, and then a computer executes 
 
20 the code, however that only applies if the human is writing pure machine codeIf.  this 
were true then our languages would be optimised for the use of the compute r.
At a finer granular level: 
¥ Human writes the code 
¥ Computer compiles the code 
¥ Computer executes the code 
 
Now we have a middle step where the computer optimises the code (e.i. compiles) for 
itÕs own use. At this point we can start to think about making the human writing code 
step as easy as possible .
As we progress through the years and higher levels of abstractions in languages, more 
and more steps in between the human and om c puter code appear, this gives us greater 
chance to optimise for the human writing the code .
If we now start to consider solving a problem, rather just writing code we also gain 
another step at the beginning: 
¥ Human creates problem solution 
¥ Human writes code 
¥ Computer compiles code 
¥ Computer executes code 
What is now worth considering is that the human is performing a compilation step. This 
is very inefficient operation for a human. This is where the DSL enters the field :
¥ Human creates problem solution in DSL 
¥ Computer compiles DSL to GPL 
¥ Computer compiles GPL 
¥ Computer executes code 
 
The examples above give a clear message that writing code for problemso   lutions is 
ultimately a human exercise and compilation, interpretation & execution is an exercise 
for the computer. The more optimisation & compilation we can get the computer to do, 
the easier we make it for the end user to create problem solutions, and the better a 
system we will have. 
 
6.3.4 Building/a/DSL/
Building a DSL is a software engineering exercise. As with all software engineering, 
there are various methods by which a piece of software can be developed, but most can 
be categorised into two types of software engineering methods: Waterfall and Agil e.
16
Waterfall is a software engineering method originally propoed s  in 1970, and although 
not named Waterfall at the time it became the template for later methods. Waterfall is a 
heavyweight process that takes each step in the development of a software system in 
order, and focuses on getting every stage correct before proceeding to the next .
17 1
Agile software engineering methods were first proposed  in 2001 . The most common 
properties of agile software methods are that they are iterative and that they rely 
heavily on interaction with the end user to ensure what is built is correc t.
 
A"Waterfall"Method"
15
Mernick  talks about several phases: 
Decision/Phase/
Making the decision to use a DSL, and following patterns to help decide whether the use 
of DS L will add value to system .
                                                               
1
 The agile manifesto can still be foundat    http://agilemanifesto.org/ (as of September 2012) 
 
21 Analysis/Phase/
18
Analysis of the problem domain by using domain analysis methods such as FAST to 
build a hierarchy of features. This hierarchy then becomes a base for thla e nguages. 
Design/Phase/
In this phase the language itself is designed. Decisions such as whether to make it formal 
or informal, or whether to piggyback off another language or create an entirely new 
syntax, are made.  
Implementation/Phase/
15
In this phase the executable language is created. Mernick  talks about the various 
options available for how to implement the language but these are very focused on 
developing around C (or similar) tool chain.  
Another  option,  not  considered  by  Mernick,  and  that  is  repeatedly  used,  is  that  of 
executing  the  language  script  as  a  function/library  call  within  another  language. 
Mernick does consider ÒinterpretersÓ but only in the sense of stanDdalone programs. 
Examples of such libraries that load and execute scripts are Jython or JRuby, both of 
which allow other languages to be executed froma    Java program. 
 
19
van Deurson  also backs this waterfall approach to development of a DSL with an inD
depth analysis phase of the problem domain to construct a feature set  that becomes the 
basis for a the language .
 
This is a very traditional method and follows a typical Waterfall model of software 
development. It invests much upDfront in analysis and design before implementation 
begins.  
 
An"Agile"Method"
Freeman describes a different method of developing an embedded DSL. In their case 
they evolved over a set of iterations, with each iteration using th elessons learnt from 
the previous one. Their process for each iteration was as follows :
¥ Examine what works and doesnÕt work in the current softwar e
¥ Make appropriate changes (producing a usable DSL) 
¥ Use the DSL (on real problems) 
This kind of iterative form of development uses experience instead of analysis to inform 
design decisions. It approaches a DSL in a more human compatible way than a clinical 
analysis by making changes and then seeing the impact of those changes upon the user s.
Waterfall"versus"Agile"
I disagree with the Waterfall model of development, especially for something as useDr
focused as a domain specific language. A DSL is typically (and is so in this case) geared 
towards a practitioner in a problem domain. It should be entirely focused on wht a
works for the end user (see Section 6.3.3), and since end users are human, logical 
analysis of what works for them is often wrong.  
 
6.3.5 What/Do/We/Want/To/Build/With/The/D/SL?
So far we have covered both artificial neural network sand domain specific languages so 
it is worth spending some time now discussing what it is we want to build with the 
domain specific language.  
 
The properties of the DSL can be broken up into two sets of features, the first feature 
that allows a neural net to be specified, and the second that allows it to be used for 
 
22 training or testing. By breaking the features into these two it can be seen that one could 
be used without the other .
 
The key features in the following sections describe broad themes that wl ilneed to be 
considered during development, each key feature will need to be broken down into 
smaller features to facilitate development.  
Definition"Key"Features"
 
Feature1 Description1
Activation1Function1 Define an activation function type (encode an algorithm 
into a reusable object) 
Neuron1Definition1 Define a neuron type within the network (in terms of itÕs 
properties such as itÕs activation function) 
Learning1Rule1Definitio1n Define a learning rule type to be used within a network 
Network1Definition1 Def ining a single neural network, with properties such as 
number  and  type  of  neurons,  learning  rules,  & 
architecture such as layers and connections 
High1Level1Architectur1e Defining ensembles or committees of networks, different 
functional groups of networks etc. 
Table131Key1features1for1the1description1of1an1 1ANN
/
Train/Test"Key"Features"
 
Feature1 Description1
Input1Data1 Describe how input data is fed into the system (the data 
source and any manipulation of the data )
Output1Data1 Descri be how output data is fed out of the system (the 
data destination, and any manipulation of the data )
Defining1Test1Dat1a Describe the location and labelling of test data, stopping 
or convergence criteria 
Execution1of1Training 1 Execute training runs 
Execution1of1Testin1g Execute testing runs 
Table141Key1features1for1the1testing1&1training1of1a1n1ANN
 
6.3.6 Summary/
There are good reasons to create and use a domain specific language, and we have seen 
two approaches to building them. We have also described some of the key features the 
DSL will have.  
Next, we will look at some libraries and tools that will make the job of building the DSL 
easier. 
 
23 6.4 Related/Work/
It is worth looking at a similar DSL to help inform any decisions that needed about or  u
20
DSL. Evolutionary Systems Description Language  (ESDL) is a DSL for specifying an 
evolutionary algorithm. Figure 8 shows an example of ESDL, the language primitives are 
shown in uppercase and the variables in lowercase. 
ESDL is a simple and straightforward language. The authorÕs intentwa   s to keep it simple 
so that it could be used to express the evolutionary algorithm as clearly as possible.  In 
this it succeeds, as can be seen inFi   gure 10, the evolutionary algorithm is clear .
However, in my opinion, the language is perhaps a little too simple. Although the high 
level algorithm is clear, there are other important algorithms that are not expressed in 
this  language  and  merely  referenced  by  name.  An  example  of  this  is  the  selection 
operators,  which  are  defined  in  Java  code  and  just  referred  to  here  (e.g. 
binary_tournament). To see these algorithms one has to revert to reading Jav a.
FROM'random_real'SELECT'500'population'YIELD'population'
BEGIN'generation_equivalent''
''''REPEAT'500'
' FROM'population'SELECT'2'parents'USING'binary_tournament'FROM'parents'SELECT'
' ' offspring'USING'crossover(per_pair_rate=0.9),'
' ' mutate(per_gene_rate=0.01)''
' FROM'offspring'SELECT'1'replacer'USING'best'
' FROM'population'SELECT'1'replacee','rest'USING'uniform_shuffle'
' YIELD'offspring','replacee'
' FROM'replacer','rest'SELECT'population''
''''END'REPEAT'
''''YIELD'population''
END'
 
Figure1101An1example1of1ESDL,1a1domain1specific1language1for1describing1genetic1alg1orithms
 
 
/
 
 
24 7 Designing/and/Implementing/the/DS/L/
 
This section describes the implementation project that created the DSL. It will describe 
the approach used, and the rationale behind it, and then the story of the software 
development. Following this, it will describe the in f al product (the DSL) and its design 
and architecture. The project was implemented by 1 person over 2 months, making this 
21
a reasonably small piece of software development.  
 
Story/ Title/ Description/ Estimate// Priority/
No./ (Story/
Points)/
1" Load"Definition" User"can"load"a"script,"and"be"given"an"object" 2" 1"
that"corresponds"to"the"ANN"
2" Activation" User"can"define"an"activation"function"in"a" 8" 2"
Definition" script"
3" Neuron" User"can"define"a"neuron"in"a"script" 8" 3"
Definition"
4" Network" User"can"define"a"network"in"a"script" 16" 4"
Definition" (input/output/connections)"
5" Activation" User"can"reuse"an"activation"function"already" 2" 5"
Reuse" defined"in"the"underlying"library"
6" Neuron"Reuse" User"can"reuse"a"neuron"from"the"underlying" 2" 6"
library"
7" Network"reuse" User"can"reuse"a"network"from"the"underlying" 2" 7"
library"
8" Training" User"can"define"a"training"algorithm"in"the" 8" 8"
Definition" script"
9" Training"Reuse" User"can"reuse"a"training"algorithm"from"the" 2" 9"
underlying"library"
10" Simple" User"can"define"how"input"and"output"data"is" 4" 10"
Input/Output" retrieved"
11" Multiple" User"can"select"which"ANN"library"to"use"as" 4" 11"
Backend" backend"
12" Define"Test" User"can"define"test"data"(for"example"labels)" 4" 12"
Data"
13" Training"" User"can"define"training"runs"in"the"script"and" 8" 13"
execute"
14" Testing" User"can"use"a"trained"ANN"to"test"on"real" 8" 14"
data"
15" Input/Output" Input"&"Output"can"be"from/to"various" 8" 15"
Source" sources"
16" Multiple" User"can"define"multiple"networks"and"how" 16" 16"
Networks" they"connect"
17" High"Level" User"can"reuse"committee"machines"and" 4" 17"
Networks" other"networks"found"in"underlying"libs"
1
Table151The1set1of1user1stories1initially1planned1for1development1(i.e.1beve folroep1mden e t1commenced)1
7.1 The/Approach/
Section 6.3.4 describes  the  benefits  that an  agile  process  can  bring  to  a  software 
22
development project, and for those reasons an agile process, Scrum  was chosen for 
this project. While it is not the goal of this thesis to fully describe Scrum, a summary of 
 
25 Story/ Title/ Description/ Estimate// Priority/
No./ (Story/
Points)/
1" Create"XOR" Create"a"XOR"feed"forward"network"using" 16" 1"
Encog"
2" Basic"Training" Allow"the"network"to"be"trained" 8" 2"
programmatically"
3" Basic"Testing" Allow"the"network"to"be"tested"directly" 8" 3"
from"the"DSL"
4" Neuroph" Allow"the"underlying"to"be"switched" 4" 4"
between"Encog"and"Neuroph"
5" Hopfield" Allow"a"Hopfield"network"to"be"defined" 2" 5"
6" Test"Hopfield" Train"and"test"a"Hopfield"network" 4" 6"
7" Tanh" Be"able"to"choose"the"type"of"activation" 4" 7"
function"
8" Other" Expand"on"the"range"of"activation"functions" 2" 8"
Activation" for"Encog"
9" Training"Type" Allow"type"of"training"to"be"selected"(add"a" 4" 9"
second"type)"
10" Training" Add"the"following"parameters"for"training:" 2" 10"
Parameters" Epocs,"restarts,"error."
11" Training"Data" Allow"training"data"to"be"defined"in"the" 4" 11"
script"
12" Testing"Data" Allow"testing"data"to"be"defined"in"the"script" 2" 12"
13" Data"Sources" Add"the"references"to"data"sources"to"the" 8" 13"
script"(CSV"&"patterns)"
14" Command"line" Run"from"a"command"line"binary" 1" 14"
15" Add"more" Add"more"activation"functions/training/" ]" 15"
networks"as"time"permits"
1
Table161The1reOplanned1set1of1user1stories1(Note1the1final1story,1"Add1more"1is1jusho t1lade 1pl r1afcoer11
individual1stories1that1will1be1added1la1ter)
 
the Scrum process and terminology is included in section 14.3 for the reader who is not 
familiar with this process. 
At the start of the project a product backlog was created and the features created and 
prioritised. These features were based on the functionality required, that had been 
discovered during the research review. This product backlog can be seen i nTable 5. 
After a short time of working on the software development it became clear that the 
initial plan needed revising. Whilst the ea f tures that were described were all valid and 
the priorities reasonable, after working with both the Encog ANN librar yand JParsec, a 
new set of features and priorities made more sense . 
The initial plan was based on desired language elements. This, I beielve, was a mistake 
and it made more sense to base the features on whole examples. The reason for this is 
that the individual language elements were not easily testable on their own, whilst 
working from entire examples were testable. ReDplanning in this way allowed Test 
Driven Development to be used , and resulted in the product backlog shown inTa   ble 6. 
7.1.1 Test/Driven/Development/
Test Driven Development (TDD) is a programming process which originates in the 
23
practices of the Extreme Programming (XP) process . TDD follows a simple iterative 
process to write code: 
1. Write an automated unit test that fails 
2. Write code that makes the test pass 
 
26 24
The central philosophy  of TDD is that the programmer is not allowed to touch the 
actual code without an automated test failing. This results in source code being built that 
has a very high level of automated test coverage. This high level of code coverage then 
has the knockDon effect of enabling refactoring and redesign at all stages of the projec t.
By iteratively creating a test that fails, and then making that test pass, source code is 
built that is well tested and designed and created through experience with se u of that 
code. This is something that more tradition up front design does not address (instea, d
relying on long designDcodeDredesign iterations or the experience of the designer). 
7.1.2 Tools/
In  this  project,  since  Java  was  the  implementation,  JUnit was  used  to  provide  the 
automated testing framework. Also, with Eclipse being used as the IDE (Integrated 
Development Environment),  a plugin called Infinitest was used. This plugin constantly 
scans the source code for changes, and when they occur runs any tests th ami t ght be 
affected. It then provides a clear visual clue to the developer as to whether all tests are 
passing, or if not, which ones fail and where.  
Using TDD wi th Scrum, and with a newba   cklog based on working examplesbe   came the 
starting point for the actual development. 
7.2 The/Development/Story/
In this section the key stages of the software development project are described. The 
intention is not to show each the implementation of each feature butto    show where the 
important decisions and designs are made .Since the project followed an agile process, 
there was no big upfront design, instead relying on practices such asth e use of design 
11 24 25
patterns , TDD , refactoring  etc. to provide an environment in which changes could 
be  easily  and  safely  made  as  needed.  The  following  sections  are  presented  in 
chronological order. 
Project"Hosting"&"Tools"
One of the main goals of this project was to make the language and code openso   urce. To 
this end a project was created at GitHub and this used as the master source code 
repository. This ensured offDsite backup during the project and full versioning using the 
Git SCM (Source Code Management). At this point it was necessary to give he t  project a 
name: Neural. 
The project site can be found at:ht   tp://github.com/mbcx4jrh/Neural 
Hello"World"(or,"defining"a"XOR"network)"
Following the process for TDD meant that it was necessary to define a test before 
writing code. The simple example of defining a feed forward network with one hidden 
layer for simulating a XOR logic gate was chosen as the starting point. A script was 
written to define such a network (see Figure 12), and then a JUnit test created that 
loaded the script and created an ANN (see Figure 11). Only at this point was actual 
source code written, and its goa lwas to make the test pass .
At this point the decision was made tocr   eate an interface for objects representing ANNs 
and that concrete implementations of this interface would be passed from a parser 
11
object, acting as a Factory . 
After the initial test passed, further additions were made t othe automated test, to start 
testing input & output so that the code could be progressed . 
 
27 network'xor'is'feedforward'{'
' layer'{'
' ' size'2'
' ' activation'input'
' }'
' layer'{'
' ' size'3'
' ' activation'sigmoid'
' ' biased'
' }'
' layer'{'
' ' size'1'
' ' activation'sigmoid'
' ' biased'
' }'
}'
Figure1121The1first1Neural1script,1used1to1build1an1ANN1capable1of1learning1the1XOR1 1operation
@Test'
public'void'testBasicXor()'{'
'''' Network'network;'
' ScriptParser'='new'ScriptParser();'
' network'='parser.parse(FileUtils.readFileToString(Òxorh1.neuralÓ);'
}'
Figure1111The1first1test1case,1which1merely1reads1a1file1and1produces1a1Network1object.1
No1other1code1has1been1written1so1this1test1fails1by1virtue1of1not11compiling.
 
Adding"Neuroph"
The first ANN features to be added wereim   plemented using the Encog ANN library. It 
was a goal of the project that the DSL could be implemented across many ANN libraries 
so the initial XOR test was replicated, andal   tered to use the Neuroph library. This was 
done by adding the following code: 
 
parser.setUnderlyingLibrary(Òneural.networks.NeurophNetworkFactoryÓ);'
 
to the test, which, of course, made it fai. lImplementing this method and the resulting 
functionality enabled the underlying ANN library to be selected . 
11
The Abstract Factory design pattern was adopted for this feature, as the function of an 
abstract factory is that concrete implementations of the factory are used in different 
situations. Here, each ANN library would have itÕs own concrete factory, andse t with the 
method  shown  above,  that  way,  integrating  an  new  ANN  library  can  be  done  by 
implementing a new factory for that library. 
Defining"A"New"Network"Type"
A second example was created, this time defining a Hopfield networ kand can be seen in 
Figure 13. Now the network factory needed to be able to produce different concrete 
networks based on the name provided in the script. To doth   is a properties file was used 
with entries such as: 
 
network.hopfield'neural.networks.encog.EncogHopfieldNetwork'
 
 
28 This property links the name of a network type ho Ò pfieldÓ in the script, to a class to 
instantiate by the network factory. Supplying such a property file for each underlying 
ANN  library  would  allow the  network  factories  to  tie  a  keyword  in  the  DSL  to an 
implementing class to instantiate. 
network'hopfield_test'is'hopfield'{'
' size'5'
}'
Figure1131The1secondNe 1 ural1script1created1for1the1purposes1of1tes1 ting.
 
Adding"Activation"Functions"
The initial feed forward networks all used the same activation function (a sigmoid) so 
new  tests  were  created  that  did  exactly  the  same  except  with  different  activation 
functions. By following the same pattern as with different network types, the properties 
files were used to tie the activation function name in the DSL to an implementing class, 
for example, the properties file: 
 
activation.sigmoid'org.engine.encog.network.ActivationSigmoid'
 
binds the identifier ÒsigmoidÓ in the DSL to the implementing class. 
Note that in the case of theEnc   og library, the activation functions could be used directly. 
This was because they all had a default constructor and so could be passed directly to 
the  network  without  trouble.  In  the  case  of  the  Neuroph  library,  it  was  more 
11
troublesome and adapters  had to be used instead. 
Training"
Similarly to activation functions, training in the first example was a fixed type. New test 
scripts  were  created  with  various  training  types,  and  the  same  pattern  as  for  the 
activation and network types was used to define a training type, for example in the 
properties file: 
 
training.backpropagation'neural.networks.encog.BackPropAdapter'
 
This binds the identifier ÒbackpropagationÓ in the DSL to the training implementation 
stated. 
At this point additional generic training parameters were also added to the language, 
allowing training to run for a max number of epochs, to a certain error rate, and finally if 
both of those fail to converge then to restart the training a certain number of time s.
Training"and"Testing"Data"
At this stage training and testing of the ANNs produced from the DSL had to happen 
programmatically, that is, passing the data directly to the network objects. To allow this 
to happen automatically, the definition of data blocks ws  a added to both training and 
testing statements within the DSL script, for example: 
 
input'{'
''1'0'1,'
''0'0'0,'
''1'1'0'
}'
 
29  
And again, this was done by adding new tests that contained these data block, sand then 
adding the code that made these tests pass .
At this point there was now a script that could define an ANN, the training method, and 
the training and testing data. Training could be simply invoked by calling :
 
network.train();'
Data"Sources"
Although defining data directly in the DSL script is useful for simple examples, for more 
realistic problems it is far more useful to be able to define a source for dat aA  . new 
version of the XOR test was created, but this time referring to  file on the system, for 
example: 
 
input'csvfile'Òscripts/xorhtest.csvÓ'
 
The same properties file pattern was used as before, linking the input type oÒc f svfileÓ to 
an implementing class through this entry in the properties file :
 
data.csvfile'neural.data.CSVFileSource'
 
This allows different type of data sources to be used, with the  second parameter to the 
statement being the identifier of the source (which in this case is file name ).
Command"Line"Use"
All usage, so far, of the Neural DSL was through its API. It would be useful to be able to 
define full scripts that contained both an ANN definition and training and testing data, 
and  run  them  from  the  command  line.  This  would  avoid  the  need  to  use  Java 
programming at all.  
A command line executable was created so that the following would load a script, create 
an ANN, train the ANN, and run test data through the ANN :
 
./neural'scripts/xorhcmdline.neural''
Documentation"
As one of the goals is to create anon   Dgoing open source project that is used by others, 
documentation is a must. This means documentation both on how to useNe   ural, and on 
how to extend it. Fortunately projects on GitHub get a wik iassociated with the project, 
so this wiki was used to provide documentation to users and developers of the languag e.
 
Hopefully the reader now has an idea of the process and journey that producedhi  ts first 
version of Neural. The story presented above was not aimed to provide a technical 
description of Neural (for this see sections 7.4 & 7.3), but to provide a sense of how the 
project progressed. The project took 2 months to get to this stage and a significant 
amount effort was spent building a code base that had a high level of automated test 
coverage.  
7.3 The/Neural/Language/ /
The DSL,  Neural, could be described in many ways, both formal andin   formal. In this 
section the DSL is described formally in section 7.3.1, and then more informally in 
sections 7.3.2 to 7.3.5.  Finally  a  full  working  example,  that  can be used  from  the 
command line is given in section7.   3.6. 
 
30 7.3.1 Language/Definition/
26
In Extended BackusDNaur Form (EBNF)  the language can be described as: 
 
neural_script'='['activation'],'network,'['training'],'['testing']'
'
network'='ÒnetworkÓ,'identifier,'ÒisÓ,'identifier,'network_block'
'
network_block'='Ò{Ó,'parameters,'{'layer'},'Ò}Ó'
'
parameters'='ÒparametersÓ,'Ò{Ó,'{'parameter'},'Ò}Ó'
'
parameter'='identifier,'double'
'
layer'='ÒlayerÓ,'Ò{Ó,'ÒsizeÓ,'double,''
''''''''ÒactivationÓ,'identifier,'['biased'],'Ò}Ó'
'
activation'='ÒactivationÓ,'identifier,'ÒisÓ,'identifier,''
'''''''''''''Ò{Ó,'{'parameter'},'Ò}Ó'
'
training'='ÒtrainingÓ,'Ò{Ò,'training_params,'Ò}Ó'
'
training_params'='ÒtypeÓ,'identifier,'
'' ' '''['ÒerrorÓ,'double,'Ò%Ó'],'
' ' '''['ÒepochsÓ,'integer'],'
' ' '''['ÒrestartÓ,'integer'],'
' ' '''['input_data,'output_data']'
'
input_data'='ÒinputÓ,'data_block'|'data_ref'
'
output_data'='ÒoutputÓ,'data_block'|'data_ref'
'
data_block'='Ò{Ó,'{'double'},'['Ò,Ó,'{'double'}'],'Ò}Ó'
'
data_ref'='identifier,'id'
'
id'='ÒÒÓ,'identifier,'ÒÓÓ''
'
testing'='ÒtestingÓ,'Ò{Ó,'input_data,'['output_ref']'
'
output_ref'='ÒoutputÓ,'data_ref'
'
identifier'='<'defined'as'Java'identifier'>'
double'''''='<'defined'as'Java'double'>'
 
The definitions for ÒidentifierÓ, ÒintegerÓ and ÒdoubleÓ have been omitted, but can be 
considered the same as the definitions in the Java language .
7.3.2 Network/Definition/
The network section of a Neural script defines how an ANN is constructed. Informally 
the network section looks like: 
 
network'[name]'is'[type]'{'
' parameters'{'
' ' [parameterhname]'[parameterhvalue]'
' ' ...'
' }'
' layer'{'
' ' size'[numberhofhnodes]'
' ' activation'[activationhname]'
' ' biased''
' }'
' ...'
}'
 
 
31 In  this  definition both  the  parameters  and  the  layer  sections  are  optiona, lwith  a 
network typically having at minimum one or the othe. rThe biased keyword is also 
optional. Table 7 shows the meaning of the possible values in the scrip t.
 
Value1 Meaning1 Examples1
[name]1 The name of the ANN (unused by Neural ) Abc123,  joe, 
xor_example 
[type]1 The type of ANN that is to be created   feedforward, 
hopfield 
[parameterOname]1 The name of a network parameter  size, length, low 
[parameterOvalue]1 The value of a network parameter (a double)  1, 0, D1.234, 0.023 
[numberOofOnodes]1 The number of nodes in the layer  1, 3, 27 
[activationOname]1 The name of an activation function (this can be either a n actual  sigmoid, threshold  
activation function of the name of an activation section in he t  
script) 
Table171Explanation1of1values1in1the1network1section1 Neour f1a1 al1script1
 
7.3.3 Activation/Definition/
The activation section of a script allows the user to refer to an activation function with a 
certain set of parameters for later use. The intention is to save repeating the same 
parameters every time they are used. This section of the script looks like the following :
 
activation'[name]'is'[type]'{'
' [parameterhname]'[parameterhvalue]'
' ...'
}'
 
Table 8 shows the meaning of the possible values. 
 
Value1 Meaning1 Example1
[name]1 The identifier to be used elsewhere in the script   my_sigmoid, abc123 
[type]1 The type of activation function  sigmoid, threshold 
[parameterOname]1 The name of an activation function parameter  high, low 
[parameterOvalue]1 The value of an activation function parameter (a double ) 1, 0, D5.23, 0.1234 
Table181Explanation1of1values1in1the1activation1section1 Neour f1a1 al1script1
 
7.3.4 Training/
The training section of a script defines the type of training and the training parameters, 
with the option of also adding training data. This section looks like the followin g:
 
training'{'
' type'[traininghtype]'
' error'[trainingherror]'
' epochs'[maxhepochs]'
' restart'[restarts]'
' input'[datahsource]'[sourcehid]'
' output'[datahsource]'[sourcehid]'
}'
 
The type parameter is the only mandatory parameter.Th   e values for these parameters 
can be seen in Table 9. Additionally the input and output data may reference the data 
directly using the following notation: 
 
input'{'
 
32 ' a1'b1'c1,'
' a2'b2'c2,'
' a2'b3'c2'
}'
 
Where a1, b1, & c1 are the first set of training data, and a2, b2 and c2 the second and e tc.
 
Value1 Meaning1 Examples1
[trainingOtype]1 The training algorithm that will be used to perform the  backpropagation, 
training  resilient_propagaton 
[trainingOerror]1 The error rate (as a percentage) at which the training will  0.1%, 0.23%, 5% 
halt 
[maxOepochs]1 The maximum number of training epochs, after which  1, 10000, 250001 
training will halt. Defaults to 100,000 
[restarts]1 If training hit the maximum number of epochs without  1, 10, 50 
reaching the required error rate, then this controls the 
number of time training is restarted after reinitialising 
the network. Defaults to 1 
[dataOsource]1 The type of data source to be used  csvfile, patternfile 
[sourceOid]1 An identifier describing the data source location. For a  Ò/User/joe/test1.csvÓ, 
file data source this might be a filename. This is enclosed  ÒxorD3Ó 
in quotes 
Table191Explanation1of1values1in1the1training1section1 Neour f1a1 al1script1
 
7.3.5 Testing/
The testing section of a script defines the data that will be passed tohe  t ANN and the 
location of that data. It also, optionally, defines where to put the output from the ANN, 
by default writing to the console. The testing section looks like the following  :
 
testing'{'
' input'[datahsource]'[sourcehid]'
' output'[datahsource]'[sourcehid]'
}'
 
The input data source can be replaced by referencing the data directly as  insection 
7.3.4, and the meaning of [dataDsource] and [sourceDid] found in Table 9. 
7.3.6 A/Full/Example/
Figure 14 shows a full example for a feed forward ANN that is capable of leanirng the 
XOR operation. This example contains itÕs own training data, and reads the test data 
from a CSV file. Although this example uses the activation section, it does so only to 
reference the activation function by a different name (and so to change the  type of 
activation function only one change is needed, rather than for each time it is used in a 
layer). 
 
33 activation'my_activation'is'sigmoid'
'
network'xorhexample'is'feedforward'{'
' layer'{'
' ' size'2'
' ' activation'input'
' ' biased'
' }'
' layer'{'
' ' size'3'
' ' activation'my_activation'
' ' biased'
' }'
' layer'{'
' ' size'1'
' ' activation'my_activation'
' }'
}'
'
training'{'
' type'resilient_propagation'
' error'0.01%'
' epochs'50000'
' restart'5'
' input'{'
' ' 0'0,'
' ' 1'0,'
' ' 0'1,'
' ' 1'1'
' }'
' output'{'
' ' 0,'
' ' 1,'
' ' 1,'
' ' 0'
' }'
}'
'
testing'{'
' input'csvfile'Òscripts/xorhtest.csvÓ'
}'
 
Figure1141A1full1example1ofNe 1au1ral1script1for1learning1a1XOR1operat1ion
7.4 The/Architecture/of/Neural /
The final result of this project is a Java library that can be used to parsNe e ural scripts 
and can instantiate and use the ANNs defined in those scripts. This section describes 
three technical architecture views of this library: the overall platform, the API from a 
users point of view, and how the underlying ANN libraries (such as Encog or Neuroph) 
are integrated. 
7.4.1 The/Neural/Platform/
The Neural platform (see Figure 15) is based on the Java Virtual Machine (JVM. )It is a 
set of libraries, usable by a user in two ways: either through a Java API (see section 
7.4.2) or through a command line binary. The platform uses the JParsec library to parse 
scripts  and  produce  an  abstract  syntax  tree  (AST, ) which  essentially  is  an  object 
representation of the script. The layer shown as theNe   ural'Core in Figure 15 uses a 
11
series of adapters and bridges  to integrate the underlying libraries into the platform. 
The Neural'Core is then able to build concrete objects, which implement theNe   ural'API 
and supply these either to the command line utility or the API user .
 
 
34 Neural Command 
Neural API
Line Tool
Neural Core
Neural AST
Other ANN Library
Neural Bridges & Adapters
Neuroph
JParsec
Encog
Java Virtual Machine
 
Figure1151The1Neural1platform1architectur 1e
7.4.2 The/Neural/API/
Neural presents a simple API to the user, consisting of just two classes (se eFigure 16). 
The ScriptParser class performs the role (as the name suggests) of parsing a script, into 
an object that implements the Network interface. This class is a factory for Network 
objects. 
The Network interface allows the user to train, and test an ANN. The script that defined 
the ANN may, or may not, have had training and testing data defined, so the it is possible 
to call both the train and test methods with, and without, test data. All data passed to the 
ANN is in the form of double arrays as these were the lowest common denomat inor.  
 
 
35 <factory>
ScriptParser
+parseScript(script: String):Network
+setUnderlyingLibrary(libraryClassName: String): void
<<instantiates>>
<interface>
Network
+train():void
+train(input:double[][], idealOutput:double[][]): void
+test():void
+test(input:double[][]): double[][]
  
Figure1161The1API1for1Neural1(as1seen1by1the11user)
 
7.4.3 Integrating/Underlying/Libraries/
Neural  integrates  the  underlying  ANN  libraries  using  the  Abstract  Factory  design 
11
pattern .  The ScriptParser class uses the abstract factory NetworkFactory to create 
Network objects, and by default the concrete instance of the abstract factory is for the 
Encog library. This factory can be changed at runtime using the setUnderlyingLibrary 
method. 
Since different libraries might have different properties for the various functions, the 
current version of Neural contains a basic form of validation for the properties that ca n
be set. Validation occurs after the script has been parsed to an AS Tand applies to 
parameter blocks for the network and the activation sections.Va   lidation is controlled by 
a validation file containing entries such as :
 
activation.threshold.mandatory'high'low'
activation.competative.optional'size'
 
Breaking down the first entry shows that it is for an activation function parameter block, 
for the threshold function, and that the high and low parameters are mandator yTh . e 
second entry describes an optional parameter for the competitive activation function .
 
7.5 Summary/
This chapter has described the project to develop the Neural language. It has described 
the way in which the project was developed, both the approach and the actions. It has 
also  shown  the  specification  of  the  end  result  as  a  technical  architecture  and  as  a 
specification of the language. The next section will describe further how to use the 
language itself and how to extend the platform .
 
36 8 Results/
This section examines the functionality of Neural.  It  firsts  looks  at  how  to  use  the 
language  both  programmatically  (i.e.  using  it  as  a  Jav alibrary  API)  and  from  the 
command  line  to  execute  scripts.  It  then  goes  on  to  describe  the  scope  of  the 
functionality current covered, before discussing how to extend the language furthe r.
8.1 Using/Neural/
Neural can be used in several ways, from using the sricpt to only define an ANN for use 
programmatically, to using the script to define and train an ANN, and then to perform 
calculations with it (testing). With all of thefo   llowing sections the script shown in Figure 
14 could be used (even though some parts of it might not be needed)  
8.1.1 Create/a/network/to/usepr / ogrammatically/
Given a neural script, it is possible to parse the script adn produce an untrained ANN, 
ready for use programmatically. To do this the following code would be nede e d: 
 
//'Load'script'from'file'(using'Apache'Commons)''
String'script'='FileUtils.readFileToString(new'File(name));'''
'
//'Create'a'Neural'parser''
ScriptParser'parser'='new'ScriptParser();'''
'
//'Parse'the'script'and'return'an'artificial'neural'network''
Network'network'='parser.parseScript(script);'
 
The network object can now be used by calling its train or test method,s and passing 
data to it. For example, to use the train method:  
 
//Load'training'data'(readXXXData()'methods'are'omitted'here)'
double[][]'input'='readInputData();'
double[][]'output'='readOuputData();'
'
//train'the'network'
network.train(input,'output);'
 
Similarly, to use the network to compute output data: 
 
//Load'input'data'
double[][]'input'='readInputData();'
'
//compute'the'output'
double[][]'output'='network.compute(input);'
 
When these methods are called, any references to training and testing data are ignored, 
and the data passed in via the methods is used instead. However, the training type and 
parameters in the training section are used, and somu   st be defined. 
8.1.2 Create/and/train/a/network/to/upr se/ ogrammatically/
If the training section of theNe   ural script contains the training data, either by reference 
or embedded, then the network can created as in section8.  1.1, and trained using the 
data in the script by calling the following method:  
 
 
 
37 //'train'the'network'using'the'data'in'the'script'
network.train();'
 
The network will be trained and can now be used programmatically. 
8.1.3 Create,/train,/and/test/to/use/programmatica/lly
A Neural script that contains all of the network, training, and testing sections that 
contain the necessary data can be used programmatically, as in the following cod e:
 
//'Load'script'from'file'(using'Apache'Commons)''
String'script'='FileUtils.readFileToString(new'File(name));'''
'
//'Create'a'Neural'parser''
ScriptParser'parser'='new'ScriptParser();'''
'
//'Parse'the'script'and'return'an'artificial'neural'network''
Network'network'='parser.parseScript(script);'
'
//'Train'the'network'using'the'data'in'the'training'section'of'the'script'
network.train();'
'
//'Test'using'the'data'in'the'testing'section'of'the'script'
network.compute();'
 
The results of passing the testing data to the network will now be delivered to the 
output device defined in the script. If no output de vice is defined then the results will be 
delivered to the console. 
An example of an output device is the memory device, which can be defined in the 
testing section of the script as :
 
testing'{'
' input'csvfile'Òxor.csvÓ'
' output'memory'Òxorh1Ó'
}'
 
11
The output of each computation is then stored in a Singleton in memory and can be 
retrieved using the key ÒxorD1Ó, as with the following code: 
 
MemoryTester'tester'='MemoryTesterStore.getInstance().retrieve(Òxorh1Ó);'
double[][]'output'='tester.getOutputData();'
8.1.4 Command/Line/use/
Neural'comes with a command line binary. This binary will load a script, create the ANN, 
train it, and then perform the testing. This has the same effect as the code in section 
8.1.3. An example with the script inFi   gure 14 would give the following output: 
 
$'./bin/neural'scripts/xorhcmdline.neural'
Creating'parser...'
Reading'file'Ôscripts/xorhcmdline.neuralÕ...'
Parsing'script...'
Training'network...'
Testing'network...'
Input:'[0.0,'0.0]''''Output:'[0.011439126875298236]''
Input:'[1.0,'0.0]''''Output:'[0.9908491756463426]''
Input:'[0.0,'1.0]''''Output:'[0.9909028910691131]''
Input:'[1.0,'1.0]''''Output:'[0.010116505779657186]'
 
 
38 8.1.5 Errors/
Three types of errors typically occur when using Neural, and all will result in a runtime 
NeuralException being thrown (or displayed, in the case of the command line binary ):
Neural"Syntax"Errors"
These types of errors occur when the syntax is used incorrectly, such as the following 
script (note ÔasÕ instead of ÔisÕ) :
 
network'joe'as'hopfield'{'
}'
 
This would result in the following :
 
Exception'in'thread'"main"'org.codehaus.jparsec.error.ParserException:'line'1,'column'13:'
is'expected,'a'encountered.'
 
Where the exception is clearly indicating the point in the file at which the error occur s.
Invalid"Property"Error"
These types of errors occur when the types of networks, training, activation functions 
etc. do not bind to an implementing class via the properties file (see section 7. 4.3). For 
example, in the following script the network type ÔhopefulÕ is not implemented :
 
network'joe'is'hopeful'{'
}'
 
This would result in the exception: 
 
Exception'in'thread'"main"'neural.NeuralException:'Unknown'property'(network.hopeful)'
 
Validation"Errors"
These types of error occur because of invalid parameters in the parameters section of 
either the activation definition or the network definition. For example:  
 
network'joe'is'hopfield'{'
' parameters'{'
' ' high'5'
' }'
}'
 
This would result in the exception: 
 
Exception'in'thread'"main"'neural.NeuralException:'Invalid'network'definition''joe''[Parameter'
'high''is'present'but'not'allowed'as'optional'or'mandantory,'Mandantory'parameter''size''is'
missing]'
 
Which states that the parameter ÔhighÕ s  i not allowed, and that a required parameter is 
missing. 
 
39 8.1.6 Changing/the/ANN/librar/y
By default, Neural, uses the Encog ANN library to implement the networks. This can 
changed by calling a method on the ScriptParser with the name of the factory for the 
alternative library. For example, to get Neural to use the Neuroph ANN library the 
following code would be used: 
 
//Create'parser'
ScriptParser'parser'='new'ScriptParser()'
'
//Set'underlying'ANN'library'
parser.setUnderlyingLibrary(Òneural.netoworks.NeurophNetworkFactoryÓ);'
 
Any scripts that are parsed would now produce networks implemented by the Neuroph 
library. 
8.2 The/Scope/of/Neural /
One of the goals of this project wasto    start up a project that would have a life beyond 
this  thesis. While  being  able  to  cover  the  full  functionality  of  both  the  Encog  and 
Neuroph ANN libraries would have been good, realistically, time constraints made this 
impossible. The Neural platform was built as an extendable platform, so expanding the 
current functionality is matter of tie, rather than design, and will continue into the 
future.  
Encog  was  chosen  as  the  reference  platform  for Neural,  with  Neuroph  also  being 
implemented across just a few features to act as a proof of concept for the expandability .
Tables 8,9, & 10 show the current feature coverage of Neural across both libraries. 
 
 
 
Network1Type1 Encog1 Neuroph1
Feed1Forward1(Perceptron)1Neural1Networ 1 k ?  ? 
Hopfield1Network1 ?  ? 
Adaptive1Resonance1Theory111(ART1 1 ) ?   
Adaline1 ?   
Bidirectional1Associative1Memory1(BAM 1)    
Boltzmann1Machne i 1    
Counterpropagation1Neural1Networ1k    
Elman1Recurrent1Neural1Netwo1rk    
? 
Jordan1Recurrent1Neural1Netwo1rk  
Neuroevolution1of1Augmenting1Topologie1s1
   
(NEAT)1
Radial1Basis1Function1Netwo1rk    
Self1Organizing1Map 1    
1
Table1101Coverage1of1ANN1types1for1Encog1and1Neuroph1(feature1set1is1from 11Encog)
 
40 Activation1Function1 Encog1 Neuroph1
? 
Bipolar1  
? 
Competitive1  
? 
Elliot1  
? 
Gaussian1  
?  ? 
Hyperbolic1Tangent1
? 
Linear1  
? 
Sine1Wave1  
?  ? 
Sigmoid1
? 
SoftMax1  
? 
Step1(threshold)1  
? 
Tangential1  
Table1111Coverage1of1activation1functions1for1Encog1and1Neuroph1(feature1set1is1from 11Encog)
Training1Method1 Encog1 Neural1
? 
ADALINE1Training1  
?  ? 
Backpropagation1
Competitive1Learning1    
?  ? 
Hopfield1Learning1
Instar1and1Outstar1Trainin 1g    
Levenberg1Marquadt1(LMT1)    
Manhattan1Update1Rule1Propagatio1n    
Nelder1Mead1Trainin1g    
?  ? 
Resilient1Propagation1
? 
Scaled1Conjugate1Trainin1g  
1
Table1121Coverage1of1training1mho etd1for1Encog1and1Neuroph1(feature1set1is1from11Encog)
 
8.3 Extending/Neural/
One of the goals is to have an extendable language, both in functionality it provides 
through network types, activation functions, and training methods, but also with he t  
ability  to  use  different  underlying  ANN  libraries.  This  section  describes  howth   ese 
extensions can implemented, and also shows how the design of the Neural platform is 
conducive to extending the syntax .
8.3.1 Extending/The/ANN/Library/Coverage/
While the method in which a particular Network Factory implements the underlying 
ANN library is entirely up to the Network Factory, both Neuroph and Encog Network 
Factories  follow  the  same  pattern.  This  section  will  describe  howto    increase  the 
coverage for the Encog Network Factory. 
Note that for adding activation functions and training types, this in highly dependent on 
how the Network Factory as been implemented.  
Adding"A"New"Network"Type"
To add a new type of ANN, a new implementation of the interface Network is reqre uid. 
The Network implementation adapts the underlying library implementation of the ANN 
11
to the Neural interface (the Adapter design pattern ). This is made easier by extending 
the abstract class AbstractNetwork, which provides a default implementation for many 
of the methods. The implementation must take care of two key methods :
 
public'void'initNetwork(NetworkDef'definition);'
public'double[][]'compute(double[][]'input);'
 
 
41 The first constructs and initialises the underlying libraryÕs ANN, and then the second 
will perform computations with the ANN .The training methods are taken care of by 
default by the AbstractNetwork, but may need overriding depending on the type of 
network. Figure 17 shows the full interface to the Network object that would need  
implementing if the AbstractNetwork were not used .
public'interface'Network'
'
' //'initialization'methods'
' public'void'initNetwork(NetworkDef'networkDefinition);'
' public'void'initTraining(TrainingDef'trainingDefinition);'
' public'void'initTesting(TestingDef'testingDefinition);'
'
' //'returns'the'name'of'the'ANN''
' public'setName();'
'
' //'returns'the'type'of'network''
' public'String'getType();'
'
' //'training'methods'
' public'void'train();'
' public'void'train(double[][]'input,'double[][]'output);'
'
' //'compute'methods'
' public'void'compute();'
' public'double[]'compute(double[]'input);'
}'
Figure1171The1interface1for1the1Network1objecNe t1iunr1al1
After implementing the Network interface, an entry must be placed in the properties file 
(for Encog, in encog.properties), that binds the network typ ein the Neural script to the 
new class. For example, if we wanted to create a new ANN implementation for  the 
Elman Recurrent Network, we might want to refer to it in the script as ÔelmanÕ, and we 
would create a new class, implementing Network, called ÔElmanNetworkÕ. The entry in 
the encog.properties file would therefore be :
 
network.elman'neural.networks.encog.ElmanNetwork'
 
After this Neural would recognize this network type and scripts such as the following 
could be used: 
 
network'abc'is'elman'{'
' ...'
}'
Adding"New"Activation"Functions"
Adding a new activation function for the Encog library merely entailsad   ding a new 
entry into the properties file. EncogÕs own activation functions all implement a default 
constructor and are passed directly to the ANN object he , nce a trivial operation to add 
them. 
Parameters are also easy to add for the Encog library. Since all activation functions 
implement a getParameter/setParameter interface, the Encog Network Factory can use 
this directly. All that is required is the corec r t validation properties adding. 
For  example,  to  add  the  Step  activation  the  class  needs  to  be  added  to  the 
encog.properties file: 
 
 
42 public'interface'TrainingAdapter'{'
'
' //'initialization'method'
' public'void'init(TrainingDef'trainingDefinition,'Network'network);'
'
' //'training'methods'
' public'void'train();'
' public'void'train(double[][]'input,'double[][]'output);'
}'
Figure1181The1interface1for1adapting1from1thede 1urlnying1library's1training1cla1 ss
activation.step'org.encog.engine.network.activation.StepActivation'
 
Then an entry for the optional parameters, is added toth   e validation.properties file: 
 
activation.step.optional'high'low'center'
 
Now the new activation function can be used inNe   ural scripts: 
 
activation'my_act'is'step'{'
' high'0.9'
' center'0.4'
' low'0.1'
}'
 
Adding"New"Training"Methods"
Training methods are also added by added with a new entry in the encog.properties, 
however, in this case there is one slight complication: in the Encog library training 
method objects are not instantiated with a default constructor. To overcome this the 
11
Adapter  design pattern is used to adapt the Encog training class to one usable by 
Neural (see Figure 18).  
8.3.2 Implementing/New/Underlying/Librarie/s
A key goal of the project was to be able to use different ANN librarie, sand Neural 
currently uses two, Encog and Neuroph. To add an ANN library so that it can be used by 
Neural requires only that the NetworkFactory interface be implemented, and the class 
name passed to the ScriptParser using the setUnderlyingLibrary method. This es r ults in 
the  parser  using  the  new  network  factory  from  that  point  on.  Of  course  it  is  also 
necessary to implement the various features of the new library through the methods 
described in section 8.3.1. 
8.4 Summary/
The chapter has described the current scope ofNe   ural and how to use and extend it. We 
have seen how the platform was designed to be simple to use and simple to extend. The 
next chapter will look at how successful this has been.  
 
43 9 Analysis/&/Evaluation/
The Neural language was evaluated using three different methods :
¥ Working through several examples that use the data from the UCI repositor y
¥ Building an ANN agent for an a Tic T ac Toe game that currently uses Sarsa and 
QDLearning reinforcement learning algorithms 
¥ Getting user feedback via a short questionnaire 
Each of these evaluation methods provides itÕs own feedback and recommendations for 
improvements, and these are collated in section 9.3. 
Additionally we will look at the overall design ofNe   ural as a pattern to be used more 
generally when implementing a DSL or API over multiple underlying libraries. And 
finally, we will look at some of the lessons learnt and mistakes made during the projec  t.
9.1 Examples/From/Neuroph/UCI//
The Neuroph project website has many tutorials that involve experiment with different 
ANN designs to process real world data from the UCI Machine Learning Repository (see 
section 12 for the addresses of these sites).Th   ese tutorials all follow a similar process in 
order  to  try  and  obtain  an  optimal  ANN  design  for  processing  the  data  (typically 
classification or function approximation). The stages of this process are :
1. Prepare the data set 
2. Create a training set  
3. Create an ANN 
4. Train the ANN 
5. Test the network to ensure it is trained properly 
Stages 3D5 are repeated until an acceptable ANN design is found.  
This process was followed for 5 of the tutorials, usingNe   ural to implement the ANNs, 
and  during  each  stage  notes  were  made  on  what  went  well  and  what  difcu filties 
occurred. These are summarized below .
Preparing"The"Data"Set"
The Neuroph examples all perform normalization on the original data from the UCI 
repository.  In  the  examples  this  is  performed  using  a  sprea dsheet  and  CSV  files 
produced, and this was replicated when using Neural. 
It was noted in all cases, that it would be an easy addition to the language to have a 
ÒnormalizingÓ  data  source,  so  that  the  original  data  could  be  used  directly.  It  was 
surprising that Neuroph does not have the feature, given its prevalence throughout the 
tutorials. 
Creating"A"Training"Set"
Neuroph allows the user to create separate training and testing sets. It assumes that the 
data is already split into these sets, and that the input and outputs are in one fi le.
Using Neural is similar, except we must go the extra step of splitting out the input and 
output data manually into separate files. It would have been better here to have the 
input and output data in one file and to be able to reference tha t.
Creating"An"ANN"
The tutorials all involved feed forward networks, and each time this stage was repeated 
the process of creating a new ANN was followed using the Neuroph GU.  I
Using Neural made the process much easier, as when changing n  a ANNs parameters a 
quick edit could be made to the file (for example changing the number of hidden nodes) 
and it simply rerun from the command line. SeeFi   gure 19 for an example of a script 
used in the tutorials. 
 
 
44 Training"The"ANN"
For both Neuroph and Neural this was a straightforward step, for Neuroph just a button 
press, and for Neural running from the command line. What did differ, however was the 
quality of the output, as Neuroph could show a graph of the error as the training 
progressed. It would be good if Neural had some sort of output to show how training 
progressed.  Additionally  it  was  noted  once  or  twice  thatNe   ural did  not  reach  the 
required error level within the default number of epochs, however it did not tell the user 
this. 
Testing"The"ANN"
When Neuroph tests the ANN it shows not just the computation results but also errors 
(include total mean square error) as well. For Neural, there is just the result of the 
computation, and the errors had to be calculated in a sprea dsheet. The actual results of 
the learning were identical.  
It would be good to have Neural produce the same kind of output as Neurop h.
Summary"
Neural worked well as a means of defining an ANN and being able to easily change and 
run. However itÕs handling of training and testing output let it down whe ncompared 
against the Neuroph tutorial. The following features would have made running these 
tutorials extremely trivial: 
¥ Normalisation of raw data 
¥ Combine input and output data in the same fil e
¥ Reporting of errors during training and testing (this could e  b to the command 
line) 
¥ Cross validation to avoid having to split training/test dat a
With these features Neural would have perform much better than the Neuroph tools .
activation'my_activation'is'sigmoid'
''
network'concrete_strength'is'feedforward'{'
' layer'{'
' ' size'14'
' ' activation'input'
' ' biased'
' }'
' layer'{'
' ' size'20'
' ' activation'my_activation'
' ' biased'
' }'
' layer'{'
' ' size'1'
' ' activation'my_activation'
' }'
}'
'
training'{'
' type'backpropagation'
' error'0.1%'
' input''csvfile'Òconcrete_train_input.csvÓ'
' output'csvfile'Òconcrete_train_output.csvÓ'
}'
'
testing'{'
' input'csvfile'Òconcrete_test.csvÓ'
}'
Figure1191An1example1of1a1script1used1while1following1the1Neuroph1examples1(to1be1run1from1the1com1mand1line)
 
45 9.2 Building/An/Agent/for/Tic/Tac/Toe /
As part of the Learning in Autonomous Systems course atTh   e University Bristol, a 
software package is used to investigate various machine learning algorithms playing a 
game of Tic Tac Toe. The software source code was written in Java and designed to be 
extendible, especially allowing additional machine learning algorithms to be added, and 
play against one another. It was decided that it would be a good idea to implement an 
ANN agent to attempt to play against other algorithms . 
The implementation for the ANN agent was simple and kept a record of all moves in a 
game for which that player finally won (up to a maximum training set size). At the end of 
each game the ANN agent would reinitialize the ANN and train from the current training 
set. When asked by the software for a move, the agent would use the current stae tof the 
board as input and select the output with the highest score. If the movwe e  re invalid 
then a random valid move would be chosen . 
The idea was for the ANN agent to learn moves that resulted in a win, given the current 
state of the board. Aby   Dproduct of this was that only valid moves would be learned.  
The architecture of the ANN was a feed forward network with 9 input and 9 output 
nodes. The number of hidden nodes could be varied, and the starting point can be seen 
in Figure 20. The code for the ANN agent can be found in the Appendix (sectio14. n  2). 
network'nn_agent'is'feedforward'{'
' layer'{'
' ' activation'input'
' ' size'9'
' }'
' layer'{'
' ' activation'sigmoid'
' ' size'18'
' }'
' layer'{'
' ' activation'sigmoid'
' ' size'9'
 } 
}'
'
training'{'
' type'resilient_propagation'
' error'0.1%'
}'
Figure1201The1Neural1script1for1the1ANN1player1in1Tic1Ta1 c1Toe
What"Worked"Well"
Creating and training the ANN was very simple with theNe   ural'API. Most of the code 
written was to convert between the Tic Tac Toe of s twareÕs representation of the board 
and the double arrays required by Neural. 
Once the code was written, training parameters and the network structure (for example, 
the number and size of hidden layers) coud  l be varied without changing any code. This 
made it very easy to experiment with .
What"Was"Difficult"
It was difficult to view what was actually happening during training. It would have been 
good to be able to see the errors during training, and also whether or no the required 
error had been reached.  
Result"
As a result of the above an ANN agent successfully played Tic Tac Toe. The ANN agent 
easily  learnt  valid  moves,  and  it  would  win  consistently  against  a  random  player. 
 
46 However against the Sarsa or QDLearning algorithms, it lost (this was not unexpeced t  as 
these algorithms learn perfect games). However, the purpose of this was not build a 
great playing ANN agent but to investigate how wellNe   ural performs in facilitating that 
build.  
9.3 Feedback/from/users/
Two groups of users were approached and asked fo rinformal feedback in the form of a 
short questionnaire. The first group was a set of existing Encog users (Neuroph users 
were considered but the coverage of functionality from that library was too low). These 
users are already using and experimenting with ANNs and range in the experience from 
student  to  professional. The  second  group  was  a  group  of  peers  on  the  Advanced 
Computer Science MSc. at The University of Bristol. Most of these had little experience 
with ANNs (ANNs were not on this years syllabus) .The results were collated and are 
given as a summary across all users. 
The"Questionnaire"
The questionnaire was presented through email and asked the following questions :
1. What is your level of experience with ANNs ?
2. How easy was it to use Neural? 
3. How steep was the learning curve for Neural features? 
4. What do you think is missing? 
5. Would you use Neural in future (assuming you are using ANNs)? 
Results"
Total of 9 replies: 
1. Range from inexperienced to very experienced, with most in the middl e
2. Easy to download. Easy to include library in own projects. Some issues with 
location of properties files. 
3. Generally a flat learning curve with using the language. People inexperienced in 
Java thought extending the language would be difficult .
4. Cross  validation.  Normalisation.  Finer  control  of  training  parameters.  More 
training methods. 
5. Approximately 60% replied positively, but mostly on the condition that it would 
be further developed and supported. 
Analysis"
The Neural language was easy to learn and accessible for all. There wereso   me slight 
problems with the packaging of the binary, but these could be easily fixedNe .  ural is a 
tool that will be used, but it must be an active project in order for it to be use d.
Features that would be desirable to add are :
¥ Cross validation 
¥ Normalisation 
¥ More training parameters (for example, learning rates )
9.4 What/Should/Be/Changed/or/Adde/d/
The evaluation has suggested several features that should be adde:d 
Cross"Validation"
27
Cross validation is a technique for utilizing a single data set for training and testing . 
One of the most common issues with usingNe   ural for real problems is a lack of training 
11
and testing analysis. This feature could be added as a decorator type design pattern to 
the existing data sources, or it could be implemented more fully within the language to 
allow more control (for example, with the number of folds) .
 
47 More"Detailed"Training"Information"
The ability to see how the error changes as the training prgr oesses is a useful feature. 
This could be accomplished easy with some command line options that output the 
information to console, or some file. It would be simple to use this for optti l ng graphs 
using whatever tool the user wished (such as gplot) .
Normalisation"
Normalisation would speed up the process of preparing the data and although it had 
initially been discarded as outside of the scope of this project, itbe   comes clear when 
running  through  the  UCI  examples  that  it  would  be  extremely  beneficial,  and im t e 
saving. It could be implemented in a similar way to the cross validation, either through a 
decorator design pattern with data sources, or through itÕs own syntax in the training 
section of a script. 
Increased"Functionality"Coverage"
The key areas to increase coverage would be with the training algorithms. Adding new 
training  algorithms  would  allow  more  experimentation,  as  would  allowing  greater 
parameter  control  on  existing  algorithms.  This  would  take  the  form  of  allowing  a 
parameters block in the training section, and adding new implementation of existing 
parameters. 
Expanding coverage across more of the Neuroph library, and completing the Encog 
coverage would also improve the usability .
Packaging"
Although not a major feature, improving the packaging wou ld increase the accessibility 
of Neural. Currently the deliverable is a whole project including jars, source code, class 
files, property files, etc. It would be good to supply a small binary only package that 
wrapped up the jars and property files in a sigl ne package. 
GUI"
When experimenting with different network designs for the UCI examples, it was helpful 
to have a GUI tool to visualize thetr   aining. Visualizing the network while looking good, 
does not actually help much with the experiments .
One direction this feature could take is to provide an IDE type environment in which 
Neural scripts  could  be  created, edited  and  run.  The  Eclipse IDE  p latform,  or  the 
NetBeans IDE platform would be ideal for this. 
9.5 Neural/As/A/Design/Pattern /
In  software  engineering  a  design  pattern  is  a  reusable  solution  to  a  commonly 
28
encountered problem , and it is possible to examine theNe   ural platform and extract the 
design as a pattern that could be applied to other simila pr r oblems. 
The"Problem"
A common set of algorithms (for example, machine learning) are implemented in a 
software library, with each algorithm implemented in a single clas. sMany such software 
libraries exist for these algorithms, however each provides a different interface. A single 
API is wanted so that these libraries can be interchanged .
The"Solution"
11
By using the Abstract Factory, and Adapter patterns  a common API can be provided 
that underneath is implemented using apa   rticular library.  
Figure 21 shows the class diagram for this pattern, and the key elements are :
¥ The Gateway class provides a means to access theAl   gorithm implementations1
¥ The Gateway class uses the AbstractFactory, and which particular concrete 
instance is used is controlled by the methodse   tUnderlyingLibrary() 1
 
48 ¥ The ConcreteFactory supplies instances of Algorithm that adapt the part of the 
SpecificLibrary that implements the required algorithm1 .
¥ A different library can be implemented by creating a new implementation of 
AbstractFactory and an Algorithm implementation for each algorithm in the 
new library1
 
This design pattern can be applied to situations when multiple libraries need to be 
substituted  for  one  another.  This  might  be  because  of  differing  capabilities, 
performance, or functionality. A particular example is to apply this to machine learning 
in general, where multiple libraries exist that implement the various machine learning 
algorithms. 
 
Gateway
+getAlgorithm(deÞnition): Algorithm
<interface> <interface>
Algorithm AbstractFactory
setUnderlyingLibrary(string)
getAlgorithm(deÞnition):Algorithm
ConcreteAlgorithm2
ConcreteFactory
Attribute
ConcreteAlgorithm1
Attribute
SpeciÞc Library
 
Figure1211A1design1pattern1for1providing1a1single1API1for1multiple1set1of11libraries
9.6 Lessons/Learned/
Any project has mistakes. Agile projects especially, embrace mistakes, and takse time to 
review and learn from these mistakes. What folo l ws is a (nonDexhaustive ) sample of the 
mistakes that were either made or narrowly avoided during this projec t.
9.6.1 Do/not/underestimate/working/with/librari/es
When planning and estimating time it is easy to underestimate the amount of time it 
might  take  to  work  out  how  to  use  a  partcu i lar  library  to  perform  some  function 
properly. Every library has it own idiosyncrasies, some are very well designed and easy 
to  use,  while  others  are  unfathomable.  Most  sit  somewhere  in  between  these  two 
extremes. The amount of time it can take to figur eout complex functionality can be large 
(and significant for a small 1 person project). It is very easy to read an API and work 
through some tutorials, only to find out later the tutorial was too simple and real world 
use is far more complicated .
The risk of this happening increases as the number of librarie sin use increase, and this 
became apparent when attempting to work with as many underlying ANN libraries at 
the beginning of this project. Learning each library take time .
 
49 29
9.6.2 No/battle/plan/survives/contact/with/the/enemy/
The famous quote from Moltke aptly applies to most software projects and there are 
many reasons why the best of plans can start to unravel once the project starts. In this 
case the set of features initially planned did not correspond easily to testable enDdtoDend 
scenarios with the libraries. Also once more information was known about the ANN 
libraries it because more obvious to build scenarios that corresponded closely with the 
supplied examples from the libraries. 
At the beginning of a project, not everything is known :
¥ The feature set can change due to experienceus   ing the software 
¥ The feature set can change due to limitationson   ly known from using libraries 
¥ There may be technical limitations or problems that are only discovered once 
the project is fully engaged 
So it becomes important to be able to review the current state of the project and, if 
necessary, reDplan and change direction accordingly .
9.6.3 The/Curse/Of/Generalit/y
It is very easy to overgeneralize an interface. In fact, it is entirely possible to generalize 
every interface until the signature looks something like: 
 
public'Object'doSomething(Object'input);'
 
I call this ÒThe Curse of GeneralityÓ.  It occurs when we start to generalize an interface, 
but then donÕt know when to stop. 
When  initially  looking  at  the  Network  interface,  considerations  were  given  to 
generalizing it to cover machine learning in general. Simplified, this looked like:  
 
public'interface'Network'{'
' double[]'compute{double[]'input);'
' void'train(double[][]'input,'double[][]'output);'
}'
 
And a generic machine learning interface might look like: 
 
public'interface'MLAlgorithm'{'
' double[]'compute(double[]'input);'
' void'train(double[][]'input','double[][]'output);'
}'
 
There isnÕt much change here, just the name of the interface. However it was then 
tempting to generalize again, so that this could appy  l to any form of data processing 
with: 
 
public'interface'DataProcessor'{'
' Data'compute(Data'data);'
' void'init(Data'data);'
}'
 
Here we have done away with the doubles and replaced with a generic Data object (and 
is only one step away from Object). We have aso l  renamed the train method to ÒinitÓ to 
reflect that it might not just be training, but initializing some processo r.
All of this, although generalizing for multiple uses, is destroying the original interface 
with each step. The power of a simple interfac ecomes from its directness and clarity, 
something the DataProcessor interface would not have if it were being used for ANNs.  
 
50 It is, of course, a trade off, but I would argue that in this case it is bette  to r keep the 
original  interface  than  to  generalise  to  the  data  processor  interface.  The  machine 
learning interface however, would be a useful generalisation .
9.6.4 Benefits/of/TDD/
Many  smaller  issues  occurred  during  the  software  development,  and some  large 
refactoring had to take place. Some redesign of the key nt i erfaces even took place at 
some points. Normally if design changes occur late in a project then the cost can be 
large. 
Test Driven Development (TDD) was a great help in reducing this cost down to the same 
as at the start of the project. Because every adit dion or alteration to the source code was 
preceded by adding or altering an automated test there existed a large set of JUnit 
automated tests. These tests were run every time a change was made and gave instant 
feedback on the correctness of the code .
With a large set of automated tests, changes could be made with full knowledge of 
whether the change has broken or preserved the features. This ability was invaluable 
when making design changes during the project. Without this ability the time spent 
ensuring that the code still worked manually would have been immense, and the final 
deliverable would probably have had many bugs .
As it stands there areno    known bugs in the code. 
9.7 Summary/
We have seen how Neural has been evaluated and that it has been a successful project. It 
is also clear how the platform needs to evolve to be more useful in futur eTh . is chapter 
has also shown a new design pattern for implementing APIs over multiple underlying 
libraries Ð something that it useful across many areasof    software development. 
 
51 10 Summary/
This project was a software development for 1 person over 2 months. During that time 
the DSL, Neural, was created, evolved, and initially released. It was has ow n  been used to 
attempt real world problems outside of its ownpr   oject (see section 9.1). The evaluation 
of the use of Neural is that it is a good base on which to build a fit for purpose language 
for the future. The amount of work needed to bringNe   ural to that level is relatively 
small, and perhaps only constitutes another months worth of work .
One criticism that might be made of this project is Òwhere is the language design?Ó and 
hopefully that has been addressed by the agile nature of the project (see sections 6. 3.4 & 
7.1). One example of the agile process driving the design through real use of the API is 
with the Network API method: 
 
public'double[]'compute(double[]'input);'
 
As to begin with, this method had the ig s nature: 
 
public'void'compute(double[]input,'double[]output);'
 
And although it is only a small change on a small part of the API it represesn how t  agile 
design occurs. The initial version of this method mirrored the equivalent method in the 
Encog API, but after using it repeatedly in automated tests, it became clear that the final 
version made more sense to a programmer. Because of the high test coverage , a key 
method in the API could be changed with confidence that nothing would brea k.
Another example of the agile process is with cross validation and normalisation. Early in 
the project these features were deliberately not included, with the rationale being that 
this was data manipulation and it would be outside the scope of the project. During 
evaluation (actually using the language), it became clear that these two features would 
be highly beneficial. Here we see the benefit again of using something in anger, and one 
regret is that at least one of the evaluation problems  could have been tried during the 
project in order to bump up the priority of these feature s.
10.1 Achieving/The/Goal/s
The primary goals of the project were :
¥ Review & Evaluate 
¥ Build & Test 
¥ Evaluate 
¥ Make Available 
These have all succeeded. A review of the background of ANNs & DSLs, combined with 
an analysis of the need for a DSL for ANNs showed what needed to be done. The DSL,' 
Neural, was built and tested on real world examples (from the UCI repository). An 
evaluation of the quality of the DSL was performed and results combined into  afeature 
list for the future. Finally,Ne   ural is available here: 
 
http://github.com/mbcx4jrh/neural 
 
One additional goal that has not been completed, yet, is to present the DSL in a journal 
paper. This will wait until the next release ofNe   ural. 
 
52 10.2 Future/Plans/
The project is hosted on GitHub and available to use. ItÕs open source and there have 
already been several volunteers to help with the future development of the language   .
These  volunteers  have  come  from the  community  of  Encog  users,  and nc i lude one 
researcher and one PhD student. It is my intention to continue o  t lead this open source 
project, starting with the list of things to add in section  9.4. 
The following features (see section 9.4) are next in line for further development: 
¥ CrossDvalidation 
¥ Normalisation 
¥ More information output during training & testing 
¥ Improved packaging 
 
 
Finally, to quote Jeff Heaton, the creator of Encog, and owner of Heaton Research : 
ÒI'like'it.Ó'
 
53 11 References/
 
 
1.  Haykin, S. Neural'Networks:'A'Comprehensive'Foundation'(2nd'Edition). (Prentice 
Hall: 1998). 
2.  Siegelmann, H. T. & Sontag, E. D. Turing computability with neural nets.Ap   plied'
Mathematics'Letters 4, 77Ð80 (1991). 
3.  Senyard, A., Kazmierczak, E. & Sterling, L. Software engineering methods for 
neural networks. 468Ð477 (2003). 
4.  Rodvold, D. M. A software development process model for artificial neural 
networks in critical applications. 5, 3317Ð3322 (1999). 
5.  Lee, C.DC. & Shih, C.DY. Time Series Prediction Using Robust Radial Basis Function 
with TwoDStage Learning Rule. 2, 382Ð387 (2007). 
6.  Shah, M. A. & Meckl, P. H. OnDline control of a nonlinear system using radial basis 
function neural networks. 6, 4265Ð4269 (1995). 
7.  Maclin, R. & Opitz, D. Popular Ensemble Methods: An Empirical Study.ar   Xiv.org 
cs.AI, (2011). 
8.  Ding, S., Li, H., Su, C., Yu, J. & Jin, F. Evolutionary artificial neural networks: a 
review. Artificial'Intelligence'Review 1Ð10 (2011). 
9.  Yao, X. Evolving artificial neural networks. 87, 1423Ð1447 (1999). 
10.  Chu, S.DL. & Hsiao, C.DC. OpenCL: Make Ubiquitous Supercomputing Possible.HP   CC 
556Ð561 (2010). 
11.  Gamma, E., Helm, R., Johnson, R. & Vlissides, JDe .  sign'Patterns:'Elements'of'
Reusable'ObjectZOriented'Software. (AddisonDWesley Professional: 1994). 
12.  Brooker, R. & Morris, D. Experience with the Compiler Compiler.The   'Computer'
Journal (1967). 
13.  Knuth, D. E. Backus Normal Form vs. Backus Naur Form. Communications'of'the'
ACM 7, 735Ð736 (1964). 
14.  Johnstone, A. & Scott, E. Generalised recursive descent parsing and follow D
determinism. Compiler'Construction 1383, 16Ð30 (1998). 
15.  Mernik, M., Heering, J. & Sloane, A. When and how to develop dma o inDspecific 
languages. Acm'Computing'Surveys 37, 316Ð344 (2005). 
16.  Royce, W. Managing the development of large software systems. (1970) .
17.  Beck, K., Beedle, M. & Van Bennekum, A. Manifesto for agile software 
development. The'Agile'É (2001). 
18.  Coplien, J., Hoffman, D. & Weiss, D. Commonality and variability in software 
engineering. Ieee'Software 15, 37Ð+ (1998). 
19.  van Deursen, A. & Klint, P. Domain Dspecific language design requires feature 
descriptions. Journal'of'Computing'and'Information'Technology 10, 1Ð17 (2004). 
20.  Dower, S. & Woodward, C. J. ESDL: a simple description language for populationD
based evolutionary computation. Proceedings'of'the'13th'annual'conference'on'
Genetic'and'evolutionary'computation 1045Ð1052 (2011). 
21.  McConnell, S. Software Project Survival Guide D Steve McConnell D Google Books. 
(2009). 
22.  Schwaber, K. Agile'Project'Management'with'Scrum. (O'Reilly Media, Inc.: 2009) .
23.  Beck, K. & Fowler, M.Pl   anning'Extreme'Programming. (AddisonDWesley 
Professional: 2001). 
24.  Janzen, D. S. & Saiedian, H. On the influence of testDdriven development on 
software design. É''Proceedings'19th'Conference'on (2006). 
25.  Fowler, M. & Beck, K. Refactoring: improving the design of existing code. (1999)   .
26.  Scowen, R. S. Extended BNFDa generic base standard. (1998) .
 
54 27.  Kohavi, R. A study of crossDvalidation and bootstrap for accuracy estimation and 
model selection. International'joint'Conference'on'artificial'intelligence (1995). 
28.  Gamma, E., Helm, R., Johnson, R. & Vlissides, JDe .  sign'Patterns.'Elements'of'
Reusable'ObjectZoriented'Software. (Addsion Wesley Longman: 1999). 
29.  graf von, H. M. Militrische'Werke,'Volume'2. (Nabu Press: 2012). 
 
 
55 12 Web/Resources/
URLs are correct and accessible at the time of writing Se ( ptember 2012). 
12.1 ANN/Libraries/
dANN    http://wiki.syncleus.com/index.php/dANN 
Encog    http://www.heatonresearch.com/encog 
Neuroph  http://neuroph.sourceforge.net/ 
PyBrain  http://pybrain.org 
PyNN    http://neuralensemble.org/trac/PyNN 
12.2 Language/Resources/
JavaCC   http://javacc.java.net/ 
JParsec   http://jparsec.codehaus.org/ 
 
12.3 Other/
UCI Machine Learning Repository  http://www.ics.uci.edu/~mlearn/ 
 
 
56 13 Abbreviations/
 
ANN  Artificial Neural Network 
API  Application Programmers Interface 
AST  Abstract Syntax Tree 
CSV   Comma Separated Value 
DSL   Domain Specific Language 
EBNF  Extended BackusDNaur Form 
GPL  General Purpose Language 
GPU  Graphical Processing Unit 
GUI  Graphical User Interface 
JVM  Java Virtual Machine 
RBF  Radial Basis Function 
 
57 14 Appendix/
14.1 Neural/API/
14.2 Tic/Tac/Toe/Agent /
The code using the Neural API is highlighted. 
import'java.io.File;'
import'java.io.IOException;'
import'java.util.ArrayList;'
import'java.util.Arrays;'
import'java.util.List;'
'
import'neural.Network;'
import'neural.ScriptParser;'
import'neural.tools.WindowedData;'
'
import'org.apache.commons.io.FileUtils;'
'
'
public'class'NeuralAgent'implements'Agent'{'
' '
' private'static'final'boolean'DEBUG'='false;'
' '
' private'Network'network;'
' private'List<Data>'episodes;'
' private'Agent'randomAgent;'
' private'TTTEnv'env;'
' private'int'trainStrength;'
' private'WindowedData'dataWindow;'
' '
' /**'
' '*'Basic'algorithm'here'is:'
' '*''
' '*'Input':'9'nodes'representing'the'board'(h1'0,'1)'
' '*''
' '*'hidden'layers'
' '*''
' '*'Output:'9'Nodes'representing'the'choice'of'next'move'
' '*''
' '*'Online'Training:'play'game'storing'all'inout/outputs.''
' '*'''''if'win'then'train'using'this'data'
' '*'''''if'lose'then'train'with'opponents'data'
' '*'@param'env'
' '*/'
' '
' public'NeuralAgent(TTTEnv'env,'int'trainStrength,'int'windowSize)'{'
' ' this.env'='env;'
' ' this.trainStrength'='trainStrength;'
' ' dataWindow'='new'WindowedData(windowSize);'
' ' initNN();'
' ' randomAgent'='new'RandomAgent(env);'
' }'
'
' @Override'
' public'Action'newEpisode(State'initialState)'{'
' ' //reset'history'
' ' episodes'='new'ArrayList<Data>();'
' ' //return'first'move'
' ' return'nextMove(initialState);'
' }'
'
' @Override'
' public'void'endEpisode(float'finalReward)'{'
 
58 ' ' out("Game'over'h'final'reward:'"+finalReward);'
' ' //decide'if'train'with'history'
' ' '
' ' if'(finalReward'>'0)''
' ' ' trainNetwork();'
' ' else'
' ' ' reverseTrain();'
' }'
'
' @Override'
' public'Action'getAction(State'state,'float'reward)'{'
' ' return'nextMove(state);'
' }'
'
' public'void'out(String'msg)'{'
' ' if'(DEBUG)'System.out.println(msg);'
' }'
' '
' public'void'out(boolean'b,'String'm)'{'
' ' if'(b)'System.out.println(m);'
' }'
' '
' private'Action'nextMove(State'state)'{'
' ' double[]'input'='getData(state);' ' '
' ' double[]'output'='network.compute(input);'
'
' ' TTTAction'action'=''getAction(output,'state);'
' ' episodes.add(new'Data(input,'getData(action)));'
' ' return'action;'
' }'
' '
' private'double[]'getData(TTTAction'action)'{'
' ' double[]'out'='new'double[9];'
' ' short'x'='action.row();'
' ' short'y'='action.col();'
' ' out[x*3+y]'='1;'
' ' return'out;'
' }'
' '
' private'double[]'getData(State'state)'{'
' ' double[]'data'='new'double[9];'
' ' TTTState's'='(TTTState)state;'
' ' for'(int'x='0;'x<3;'x++)'{'
' ' ' for'(int'y=0;'y<3;'y++)'{'
' ' ' ' data[x*3+y]'='map(s.get(x,'y));'
' ' ' }'
' ' }'
' ' return'data;'
' }'
' '
' private'double'map(TTTSymbol'sym)'{'
' ' switch'(sym)'{'
' ' case'BLANK:'
' ' ' return'0;'
' ' case'O:'
' ' ' return'h1;'
' ' case'X:'
' ' ' return'1;'
' ' default:'
' ' ' return'0;'
' ' ' '
' ' }'
' }'
' '
' private'TTTAction'getAction(double[]'output,'State'state)'{'
' ' int'm'='0;'
' ' double'max'='Double.MIN_VALUE;'
' ' out("Net'chose:'"+Arrays.toString(output));'
' ' for'(int'i=0;'i<output.length;'i++)'{'
 
59 ' ' ' if'(output[i]'>'max)'{'
' ' ' ' max'='output[i];'
' ' ' ' m'='i;'
' ' ' }'
' ' }'
' ' out("..'so'move'is'"+m);'
' ' '
' ' short'x'='(short)'(m'%'3);'
' ' short'y'='(short)'(m'/'3);'
' ' out("Next'move:'"+x+",'"+y);'
' ' TTTAction'action'='new'TTTAction(x,'y);'
' ' if'( valid(action,'state))'{'
' ' ' out(false,"Net'chose'invalid'action");'
' ' ' return'(TTTAction)randomAgent.getAction(state,'0);'
' ' }'
' ' else'{'
' ' ' out(false,'"Net'chose'good ");'
' ' ' return'action;'
' ' }'
' }'
' '
' private'boolean'valid(Action'action,'State'state)'{'
' ' Action[]'possibleMoves'='env.getActionArray(state);'
' ' return'Arrays.asList(possibleMoves).contains(action);'
' }'
'
' private'void'trainNetwork()'{'
' ' int'i=trainStrength;'
' ' while'(ihh'>0)'{'
' ' ' out("Training'network...");'
' ' ' for'(Data'data:'episodes)'{'
' ' ' ' dataWindow.addData(data.input,'data.output);'
' ' ' }'
' ' }'
' ' network.train(dataWindow.getInputData(),'dataWindow.getOutputData());'
' }'
' '
' private'void'reverseTrain()'{'
' ' reverseEpisodes();'
' ' trainNetwork();'
' }'
' '
' private'void'reverseEpisodes()'{'
' ' List<Data>'newEps'='new'ArrayList<Data>();'
' ' double[]'in;'
' ' for'(Data'd:'episodes)'{'
' ' ' in'='d.input;'
' ' ' for'(int'i=0;'i<in.length;'i++)'{'
' ' ' ' in[i]'='h'in[i];'
' ' ' }'
' ' ' newEps.add(new'Data(in',d.output));'
' ' }'
' ' episodes'='newEps;'
' }'
'
' private'void'initNN()'{'
' ' String'script'='null;'
' ' try'{'
' ' ' script'='FileUtils.readFileToString(new'File("first.neural"));'
' ' }'
' ' catch'(IOException'e)'{'
' ' ' e.printStackTrace();'
' ' ' System.exit(h1);'
' ' }'
' ' '
' ' ScriptParser'parser'='new'ScriptParser();'
' ' network'='parser.parseScript(script);'
' }'
' '
 
60 ' class'Data'{'
' ' public'Data(double[]'input,'double[]'output)'{'
' ' ' this.input'='input;'
' ' ' this.output'='output;'
' ' }'
' ' '
' ' double[]'input;'
' ' double[]'output;'
' }'
'
}'
'
14.3 A/Very/Short/Summary/of/Scru/m
This project used the Scrum development methodology. The best place to learn about 
Scrum is from the online guides maintained by the creators of Scrum: 
¥ http://www.scrum.org/ 
 
This appendix will not attempt to justify and detail all of Scrum, but instead, to cover 
terminology and basic process that will be used on this project. The intent is to gi ve the 
reader a crash course introduction should they previously had no contact with Scrum   .
 
14.3.1 Basic/Concept/
Scrum is an agile methodology. Like most agile methodologies, it is iterative. Scrum calls 
its iterations Sprints. A sprint can last any amount ofti me, normally between a week and 
30 days, but is always fixed in length for the duration of the project. In this project, 
sprints of 1 week was be used. 
 
14.3.2 User/Stories/
The features of a product are described byus   er'stories. A user story is a description of a 
feature from a userÕs (endDuser, or external system) point of view. A single user story 
delivers a feature to the end product .
 
14.3.3 Backlogs/
Scrum uses two backlogs, known as the product'backlog and the sprint'backlog. The 
product backlog is a list of allus   er stories for the product, and is prioritised according to 
the desirability of the user story in the final product. The sprint backlog is a list of user 
stories to be implemented in a sprint .
14.3.4 Estimation/&/Velocity/
User stories are estimated when added o  t the product backlog. They are assigned a story 
point value. Story points represent the complexity of a story and how much work is 
expected to complete it. 
When a sprint has been completed the number of story points that have been completed 
can be added up to give a value known as thesp   rint'velocity. This velocity is used to 
predict  the  velocity  in  future  sprints,  and  determine  which  user  stories  will  be 
completed at what time. 
14.3.5 Burndown/Charts/
A burndown'chart is a graph showing the number of story point sleft on the yDaxis and 
the date on the xDaxis. There are two types of burndown charts:pr   oduct'burndown and 
sprint'burndown charts, and they are different only in the scope of the Dxaxis. 
 
61 Burndown charts are ScrumÕs primary way of predicting and visualisng i  development 
progress. Velocities can be shown and end dates calculated. The power of the burndown 
chart relies on it being always up to date, and in many teams I have worked with they 
would draw it by hand on a white board on the wall next to them   .
An example of a simple burndown chart can be seen inFi gure 22. 
14.3.6 The/Process/
Each sprint follows the following process :
Sprint"Planning"
¥ During  sprint'planning users  stories  are  taken  from  the  product  backlog  in 
priority order.  
¥ Each story is broken into tasks and the tasks estimated (in hours )
¥ When enough tasks to fill the developers time have been assigned then the 
planning ends 
Development"
During development the developers work through their task. Every day begins with a 15 
minute long meeting known as a Scrum. The scrum is a simple meeting covering only 3 
things: 
¥ What did I do yesterday 
¥ What am I going to do today 
¥ What impediments might prevent me from working  
At this time the sprint burndown chart is also updated .
Review"&"Retrospective"
At the end of the sprint all the completed stories are demonstrated, evaluated and 
confirmed. Any incomplete stories are returned to the backlog. At time the backlog may 
be reprioritised, or stories added or removed, based on the outcome of the revw. ie 
Also, a process called the retrospective takes place. This is intended to examine how the 
process has performed over the last sprint, and decide if any changes need to take plac e.
14.3.7 Summary/
The information in the sections above should have familiarised te  h reader with the 
central concepts of Scrum. Further reading is recommended if the reader wishes to fully 
understand it. 
 
 
 
Product1Burndown1
 
120 
 
 
100 
80 
60 
Story Points 
40 
Ideal 
20 
0 
0  5  10  15 
Sprint1
Figure1221A1simple1product1burndown1ch1 art
 
62 
Story1Points1Remaining1